---
title: "Modern Data Mining, HW 2"
author:
- Group Member 1
- Group Member 2
- Group Member 3
date: 'Due: 11:59 PM,  Sunday, 02/22'
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
  word_document:
    toc: yes
    toc_depth: '4'
urlcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.width=8, fig.height=4)
options(scipen = 0, digits = 3)  # controls base R output
# check if you have ISLR package, if not, install it
if(!require('pacman')) {install.packages('pacman')}
pacman::p_load(ISLR, tidyverse, data.table, leaps) # add the packages needed!!!
```


\pagebreak

# Case study 1: Self-esteem 

Self-esteem generally describes a person's overall sense of self-worthiness and personal value. It can play significant role in one's motivation and success throughout the life. Factors that influence self-esteem can be inner thinking, health condition, age, life experiences etc. We will try to identify possible factors in our data that are related to the level of self-esteem. 

In the well-cited National Longitudinal Study of Youth (NLSY79), it follows about 13,000 individuals and numerous individual-year information has been gathered through surveys. The survey data is open to public [here](https://www.nlsinfo.org/investigator/). Among many variables we assembled a subset of variables including personal demographic variables in different years, household environment in 79, ASVAB test Scores in 81 and Self-Esteem scores in 81 and 87 respectively. 

The data is store in `NLSY79.csv`.

Here are the description of variables:

**Personal Demographic Variables**

* Gender: a factor with levels "female" and "male"
* Education05: years of education completed by 2005
* HeightFeet05, HeightInch05: height measurement. For example, a person of 5'10 will be recorded as HeightFeet05=5, HeightInch05=10.
* Weight05: weight in lbs.
* Income87, Income05: total annual income from wages and salary in 2005. 
* Job87 (missing), Job05: job type in 1987 and 2005, including Protective Service Occupations, Food Preparation and Serving Related Occupations, Cleaning and Building Service Occupations, Entertainment Attendants and Related Workers, Funeral Related Occupations, Personal Care and Service Workers, Sales and Related Workers, Office and Administrative Support Workers, Farming, Fishing and Forestry Occupations, Construction Trade and Extraction Workers, Installation, Maintenance and Repairs Workers, Production and Operating Workers, Food Preparation Occupations, Setters, Operators and Tenders,  Transportation and Material Moving Workers
 
**Household Environment**
 
* Imagazine: a variable taking on the value 1 if anyone in the respondent’s household regularly read magazines in 1979, otherwise 0
* Inewspaper: a variable taking on the value 1 if anyone in the respondent’s household regularly read newspapers in 1979, otherwise 0
* Ilibrary: a variable taking on the value 1 if anyone in the respondent’s household had a library card in 1979, otherwise 0
* MotherEd: mother’s years of education
* FatherEd: father’s years of education
* FamilyIncome78

**Variables Related to ASVAB test Scores in 1981**

Test | Description
--------- | ------------------------------------------------------
AFQT | percentile score on the AFQT intelligence test in 1981 
Coding | score on the Coding Speed test in 1981
Auto | score on the Automotive and Shop test in 1981
Mechanic | score on the Mechanic test in 1981
Elec | score on the Electronics Information test in 1981
Science | score on the General Science test in 1981
Math | score on the Math test in 1981
Arith | score on the Arithmetic Reasoning test in 1981
Word | score on the Word Knowledge Test in 1981
Parag | score on the Paragraph Comprehension test in 1981
Numer | score on the Numerical Operations test in 1981

**Self-Esteem test 81 and 87**

We have two sets of self-esteem test, one in 1981 and the other in 1987. Each set has same 10 questions. 
They are labeled as `Esteem81` and `Esteem87` respectively followed by the question number.
For example, `Esteem81_1` is Esteem question 1 in 81.

The following 10 questions are answered as 1: strongly agree, 2: agree, 3: disagree, 4: strongly disagree

* Esteem 1: “I am a person of worth”
* Esteem 2: “I have a number of good qualities”
* Esteem 3: “I am inclined to feel like a failure”
* Esteem 4: “I do things as well as others”
* Esteem 5: “I do not have much to be proud of”
* Esteem 6: “I take a positive attitude towards myself and others”
* Esteem 7: “I am satisfied with myself”
* Esteem 8: “I wish I could have more respect for myself”
* Esteem 9: “I feel useless at times”
* Esteem 10: “I think I am no good at all”

## Data preparation

Load the data. Do a quick EDA to get familiar with the data set. Pay attention to the unit of each variable. Are there any missing values? 

```{r include=FALSE}
# Load the data
temp <- read.csv('data/NLSY79.csv', header = T, stringsAsFactors = F)

# Visualise the structure of the data
str(temp)

# Check for missing values
any(is.na(temp))

# Provide summaries of the data
summary(temp)
levels(as.factor(temp$Job05))
table(as.factor(temp$Job05))
```


## Self esteem evaluation

**Let concentrate on Esteem scores evaluated in 87. **

0. First do a quick summary over all the `Esteem` variables. Pay attention to missing values, any peculiar numbers etc. How do you fix problems discovered if there is any? Briefly describe what you have done for the data preparation. 

```{r include=FALSE}
# The following 10 questions are answered as 1: strongly agree, 2: agree, 3: disagree, 4: strongly disagree
summary(temp$Esteem87_1) # “I am a person of worth”
any(is.na(temp$Esteem87_1))

summary(temp$Esteem87_2) # “I have a number of good qualities”
any(is.na(temp$Esteem87_2))

summary(temp$Esteem87_3) # “I am inclined to feel like a failure”
any(is.na(temp$Esteem87_3))

summary(temp$Esteem87_4) # “I do things as well as others”
any(is.na(temp$Esteem87_4))

summary(temp$Esteem87_5) # “I do not have much to be proud of”
any(is.na(temp$Esteem87_5))

summary(temp$Esteem87_6) # “I take a positive attitude towards myself and others”
any(is.na(temp$Esteem87_6))

summary(temp$Esteem87_7) # “I am satisfied with myself”
any(is.na(temp$Esteem87_7))

summary(temp$Esteem87_8) # “I wish I could have more respect for myself”
any(is.na(temp$Esteem87_8))

summary(temp$Esteem87_9) # “I feel useless at times”
any(is.na(temp$Esteem87_9))

summary(temp$Esteem87_10) # “I think I am no good at all”
any(is.na(temp$Esteem87_10))
```

The first thing I did was create a summary of all the data to provide basic insights into the distribution of the Esteem_87 scores. After this I checked for missing values (of which there are none) and exmined the intepretation of the scores more carefully. From this, I understood that there were some questions which were framed in such a way that higher scores indicated higher levels of self-esteem, and other questions which were framed in such a way that lower score indicated higher self-esteem. This needs to be standardised across all the questions to ensure easy comparison across the different questions in Esteem_87.


1. Please note that higher scores on Esteem questions 1, 2, 4, 6, and 7 indicate higher self-esteem, whereas higher scores on the remaining questions suggest lower self-esteem. To maintain consistency, consider reversing the scores of certain Esteem questions. For example, if the esteem data is stored in `data.esteem`, you can use the code `data.esteem[, c(1, 2, 4, 6, 7)] <- 5 - data.esteem[, c(1, 2, 4, 6, 7)]` to invert the scores.

```{r}
# This will ensure that higher scores reflect higher self-esteem.
temp$Esteem87_1 <- 5 - temp$Esteem87_1
temp$Esteem87_2 <- 5 - temp$Esteem87_2
temp$Esteem87_4 <- 5 - temp$Esteem87_4
temp$Esteem87_6 <- 5 - temp$Esteem87_6
temp$Esteem87_7 <- 5 - temp$Esteem87_7
```

To fix this, I identified questions which were framed in a positive way (Questions 1, 2, 4, 6 and 7). This meant that lower scores ("Strongly Agree") indicated higher self-esteem. I inverted these scores, creating a standardised measure where higher scores across all questions indicated higher self-esteem.

2. Write a brief summary with necessary plots about the 10 esteem measurements.

```{r}
esteem_vars <- c("Esteem87_1", "Esteem87_2", "Esteem87_3","Esteem87_4", "Esteem87_5", "Esteem87_6","Esteem87_7", "Esteem87_8", "Esteem87_9","Esteem87_10")

par(mfrow = c(2,5), mar = c(3,3,2,1))

for (v in esteem_vars) {
  counts <- table(factor(temp[[v]], levels = 1:4))
  
  barplot(counts,
          main = v,
          xlab = "",
          ylab = "Count",
          col = "orange")
}

par(mfrow = c(1,1))

```

Esteem87_1 through to Esteem87_5 are highly left-skewed, meaning that the vast majority of tests scores are 3 and 4, and with means of 3.6, 3.58, 3.5, 3.53 and 3.41 respective Whilst the remaining Esteem87_6 through to Esteem87_10 are still left-skewed, they are to a lesser extent, with means of 3.28, 3.1, 3.06 and 3.37.


3. Do esteem scores all positively correlated? Report the pairwise correlation table and write a brief summary.

```{r include=FALSE}
esteem_vars <- c("Esteem87_1", "Esteem87_2", "Esteem87_3","Esteem87_4", "Esteem87_5", "Esteem87_6","Esteem87_7", "Esteem87_8", "Esteem87_9","Esteem87_10")
esteem_data <- temp[esteem_vars]

cor_matrix <- cor(esteem_data, use = "pairwise.complete.obs")
cor_matrix
```

All of the scores are positively correlated, with a minimum correlation between Esteem87_1 & Esteem87_8 (0.273) and Esteem87_1 & Esteem87_9 (0.236), and a maximum between Esteemed87_1 & Esteemed87_2 (0.704).


4. PCA on 10 esteem measurements. (centered but no scaling)

    a) Report the PC1 and PC2 loadings. Are they unit vectors? Are they orthogonal?
    
    ```{r include=FALSE}
    pca_result <- prcomp(esteem_data, scale. = TRUE)
    pca_result$rotation[, 1:2]    
    ```
    Yes, both PC1 and PC2 loadings are orthogonal, unit vectors.
  
    b) Are there good interpretations for PC1 and PC2? (If loadings are all negative, take the positive loadings for the ease of interpretation)
  
    Loadings are direction vectors that define each PC. Large absolute loadings indicate a strong contribution, and the signs indicate in which direction they move relative to each other. In this case, all the PC1 scores are positive, indicating that all the variables move in the same directon together. Furthermore, Esteem87_6 (0.347) and Esteem87_2 (0.333) are the most significant loadings. Looking at PC2, we see that Esteem87_1, Esteem87_2, Esteem87_4 and Esteem87_6 are negative whilst the remaining are positive, indicating that these two sets of variables move in contrasting directions. Furthermore, Esteem87_9 (0.4917), Esteem87_1 (-0.4452) and Esteem87_2 (-0.4283) are the most significant loadings.
    
    c) How is the PC1 score obtained for each subject? Write down the formula.
    
    The PC1 score for variable i is obtained as a linear combination of the standardised Esteem87 variables, each weighted by their corresponding loadings. In this case, the PC1 score is obtained using the formula: PC1i = 0.324Zi1 + 0.333Zi2 + 0.322Zi3 + ... + 0.318Zi10.
    
    d) Are PC1 scores and PC2 scores in the data uncorrelated? 
    
    Yes, the PC1 and PC2 scores are uncorrelated because PC1 is orthogonal to PC2.
    
    e) Plot PVE (Proportion of Variance Explained) and summarize the plot. 
    
    ```{r}
    eigenvalues <- pca_result$sdev^2
    prop <- eigenvalues / sum(eigenvalues)
    
    plot(1:length(prop), prop,
     type = "b",
     xlab = "Principal Component",
     ylab = "Proportion of Variance Explained",
     main = "Scree Plot",
     pch = 19)
    ```
    
    f) Also plot CPVE (Cumulative Proportion of Variance Explained). What proportion of the variance in the data is explained by the first two principal components?
    
    ```{r}
    plot(1:length(prop), cumsum(prop),
     type = "b",
     xlab = "Principal Component",
     ylab = "Cumulative Proportion of Variance Explained",
     main = "Cumulative PVE",
     pch = 19)
    ```
  
    From this, we can see that 60% of the variance is explained by the first two variables.
    
    g) PC’s provide us with a low dimensional view of the self-esteem scores. Use a biplot with the first two PC's to display the data. Give an interpretation of PC1 and PC2 from the plot. 
    
    ```{r}
    biplot(pca_result, scale = 0, cex = 1.2)
    ```
    
    From this, we can see that all loadings for PC1 (x-axis) point in the positive direction, indicating a positive level of self-esteem across all variables. Since there are no extreme loadings weightings, PC1 is essentially an average of all of the loadings, and as such, can be interpreted as a general level of self-esteem.
    
    Conversely, looking at PC2 (y-axis) we see that the loadings go in both the positive and negative directions, breaking the variable loadings into two different groups. This is likely visualise the different effects of positively-worded and negatively-worded questions. With the exception of Esteem87_7 "I am satisfied with myself", all of the negatively-worded questions, reflect better (positive) self-esteem compared with positvely-worded questions.
    

5. Apply k-means to cluster subjects on the original esteem scores

    a) Find a reasonable number of clusters using within sum of squared with elbow rules.
    
    ```{r}
    pc_data <- pca_result$x[, 1:2]
    temp$Esteem_PC1 <- pca_result$x[, 1]
    
    wss <- numeric(10)

    for (k in 1:10) {
      kmeans_result <- kmeans(pc_data, centers = k, nstart = 20)
      wss[k] <- kmeans_result$tot.withinss
    }
    
    plot(1:10, wss,
     type = "b",
     xlab = "Number of Clusters (k)",
     ylab = "Total Within-Cluster Sum of Squares",
     main = "Elbow Method")
    ```
    
    Looking at the Total Within-Cluster Sum of Squares, we can identify an elbow at 3 clusters
    
    b) Can you summarize common features within each cluster?
    
    ```{r}
    set.seed(123)
    cluster_cols <- c("orange", "blue", "gold")

    kmeans_result <- kmeans(pc_data,
                            centers = 3,
                            nstart = 25)
    
    kmeans_result$size
    kmeans_result$centers

    plot(pc_data,
     col = cluster_cols[kmeans_result$cluster],
     pch = 19,
     xlab = "PC1",
     ylab = "PC2",
     main = "K-means Clustering on PC1 and PC2")

    points(kmeans_result$centers,
       col = "black",
       pch = 8,
       cex = 2,
       lwd = 2)
    ```
    
    Cluster 1 contains 843 observations and is centred at (-2.3627, 0.6000); Cluster 2 contains 697 observations and is centred at (-0.0473, -1.1300); and Cluster 3 contains 891 observations and is centred at (2.2725, 0.3160).
    
    Going of my interpretation of PC1 as overall self-esteem, we can then classify Cluster 1 as low self-esteem, Cluster 2 as average self-esteem and Cluster 3 as high self-esteem. When examining PC2 above, we suggested that it may be the differing tone in which the questions were framed (positive and negative). Clusters 1 and 3 contain mostly positive values with some negative values, whereas Cluster 2 contains mostly negative values. This interpretation does not apply to these clusters because there are three clusters, rather than 2, and each cluster contains a range of positive and negative values. As such, this clusters around some factor impacting PC2, however, we were unable to find a clear interpretation.
    
    c) Can you visualize the clusters with somewhat clear boundaries? You may try different pairs of variables and different PC pairs of the esteem scores.

    Note, in this case, we have chosen to only cluster around PCs due to the potential for multicollinearity between variables in the data and the presence of unwanted noise.

    ```{r}
    pc_data <- pca_result$x[, c(1, 3)]
  
    set.seed(123)
    cluster_cols <- c("orange", "blue", "gold")

    kmeans_result <- kmeans(pc_data,
                            centers = 3,
                            nstart = 25)
    
    kmeans_result$size
    kmeans_result$centers

    plot(pc_data,
     col = cluster_cols[kmeans_result$cluster],
     pch = 19,
     xlab = "PC1",
     ylab = "PC2",
     main = "K-means Clustering on PC1 and PC3")

    points(kmeans_result$centers,
       col = "black",
       pch = 8,
       cex = 2,
       lwd = 2)
    ```
    ```{r}
    pc_data <- pca_result$x[, c(2, 3)]
  
    set.seed(123)
    cluster_cols <- c("orange", "blue", "gold")

    kmeans_result <- kmeans(pc_data,
                            centers = 3,
                            nstart = 25)
    
    kmeans_result$size
    kmeans_result$centers

    plot(pc_data,
     col = cluster_cols[kmeans_result$cluster],
     pch = 19,
     xlab = "PC1",
     ylab = "PC2",
     main = "K-means Clustering on PC2 and PC3")

    points(kmeans_result$centers,
       col = "black",
       pch = 8,
       cex = 2,
       lwd = 2)
    ```

6. We now try to find out what factors are related to self-esteem? PC1 of all the Esteem scores is a good variable to summarize one's esteem scores. We take PC1 as our response variable. 

    a) Prepare possible factors/variables:
    
       Firstly, we have conducted PCA on the ASVAB dataset, extracting PC1 scores and adding them to the dataset as a general level of intelligence.

    ```{r}
    ASVAB_vars <- c("AFQT", "Coding", "Auto", "Mechanic", "Elec", "Science", "Math", "Arith", "Word", "Parag", "Number")
    ASVAB_data <- temp[ASVAB_vars]
    
    pca_result <- prcomp(ASVAB_data, scale. = TRUE)
    temp$Intelligence <- pca_result$x[, 1]
    ```
    
    Next, we will create a BMI variable to summarise an individual's body height and weight.
    
    ```{r}
    temp$Height_in <- temp$HeightFeet05 * 12 + temp$HeightInch05
    temp$BMI <- (temp$Weight05 * 0.453592) / ((temp$Height_in * 0.0254)^2)
    ```

    Finally, we are going to remove the unwanted variables from the dataset (specifically, Esteem81 scores, AFQT scores except for AFQT). The primary reason for this is that these variables are already described through other variables such as Intelligence or Esteem, or they are not needed (like Esteem81). Note, we have decided to keep Education05 and Job05, despite being after the date that the self-esteem scores were collected because it is likely that the education and job will be the same.

    ```{r include=FALSE}
    temp <- temp %>%
        select(-"Esteem81_1", -"Esteem81_2", -"Esteem81_3", -"Esteem81_4", -"Esteem81_5", -"Esteem81_6", -"Esteem81_7", -"Esteem81_8", -"Esteem81_9", -"Esteem81_10", -"Science", 
               -"Arith",-"Word", -"Parag", -"Number", -"Coding", -"Auto", -"Math", -"Mechanic", -"Elec", -"AFQT", -"HeightFeet05", -"HeightInch05", -"Esteem87_1", -"Esteem87_2",
               -"Esteem87_3", -"Esteem87_4", -"Esteem87_5", -"Esteem87_6", -"Esteem87_7", -"Esteem87_8", -"Esteem87_9", -"Esteem87_10", -"Height_in", -"Weight05", -"Income05")
    
    temp <- temp %>%
      mutate(Job05 = as.factor(Job05))
    
    str(temp)
    ```

    Following the data preparation, we will conduct some EDA to gain a sense of the structure of the final dataset as well as the distribution of each variable.

    ```{r include=FALSE}
    summary(temp)
    ```
          
    b)   Run a few regression models between PC1 of all the esteem scores in 87 and suitable variables listed in a). Find a final best model with your **own clearly defined criterion**. 

    First we will find the quantitative factors that are most significant to modelling self-esteem. To do this, we will remove Job05 from the dataset and perform, stepwise and exhaustive selection to find the optimal model (the model that minimises RSE and maximises adjusted r-squared). Esteem_PC1 will be out dependent variable and the other factors our independent variables.
    
    ```{r include=FALSE}
    temp_nj <- temp %>%
      select(-Job05)
    ```

    We first conduct a backward step regression model which starts with all the variables and removes the least significant, until removing more variables does not improve the performance of the model.

    ```{r include=FALSE}
    full_model <- lm(Esteem_PC1 ~ . - Subject, data = temp_nj)
    backward_step_model <- step(full_model, direction = "backward", trace = 0)
    summary(backward_step_model)
    formula(backward_step_model)
    ```
    
    Next, we will conduct a forward step-wise regression and compare the effectiveness of the model.

    ```{r include=FALSE}
    null_model <- lm(Esteem_PC1 ~ 1, data = temp_nj)
    forward_step_model <- step(null_model, scope = formula(full_model), direction = "forward", trace = 0)    
    summary(forward_step_model)
    formula(forward_step_model)  
    final_model <- forward_step_model
    ```

    Finally, we will conduct an exhaustive search, which invovles testing every possible combination of independent variables in a regression model, and selecting the one with the lowest AIC.

    ```{r include=FALSE}
    df_exh <- temp[, c("Esteem_PC1",
                   "Education05","Income87",
                   "Imagazine","Inewspaper","Ilibrary",
                   "MotherEd","FatherEd","FamilyIncome78",
                   "Intelligence","BMI")]

    df_exh <- na.omit(df_exh)
    
    exh <- regsubsets(Esteem_PC1 ~ ., 
                  data = df_exh,
                  nvmax = ncol(df_exh) - 1,
                  method = "exhaustive")

    summary_exh <- summary(exh)

    n <- nrow(df_exh)
    rss <- summary_exh$rss
    k <- 1:length(rss)

    aic_vector <- n * log(rss/n) + 2 * (k + 1)
    
    best_size <- which.min(aic_vector)
    best_vars <- names(coef(exh, id = best_size))[-1]
    
    best_formula <- as.formula(
      paste("Esteem_PC1 ~", paste(best_vars, collapse = " + "))
    )
    
    best_model <- lm(best_formula, data = df_exh)
    summary(best_model)
    ```

    All of these processes came up with the same model: -2.52 + 0.116(Education05) + 0.0000188(Income87) + 0.231(Inewspaper) + 0.168(Ilibrary) + 0.0273(MotherEd) + 0.138(Intelligence). This has an adjusted r-squared of 0.118, meaning 11.8% of variation in Esteem_PC1 can be explained by variation in the independent variables, accounting for the number of independent variables. Furthermore, we calculate an F-statistic of 55 and a corresponding p-value less than 2x10^-16, indicating that the overall model is significant in explaining variation in the dependent variable. Finally, we calculate a residual standard error of 2.03, indicating that on average, datapoints are 2.03 units away from the regression line.

    Next, we will analyse the significance of the qualitative variable, Job05. First we will establish a baseline Job, Executive, Administrative and Managerial Occupations.

    ```{r include=FALSE}
    baseline <- names(sort(table(temp$Job05), decreasing = TRUE))[1]

    temp <- temp %>%
      mutate(Job05 = relevel(Job05, ref = baseline))
    
    baseline
    ```

    We will then conduct a one-way ANOVA to analyse the significance of the Job05 variable on Esteem_87. This tests to see if there is a significant difference between the coefficients of the different job categories.

    ```{r include=FALSE}
    job_reg <- lm(Esteem_PC1 ~ Job05, data = temp)
    summary(job_reg)
    anova(job_reg)
    ```
    
    Looking at this, we get an F-statistic of 6.24 and a corresponding p-value less than 2x10^-16, indicating that there is significant reason to believe that the coefficients of all the jobs are not equal. Thus, we can then add this job to the optimal regression identified above and test to see if the resulting model is better.
    
    ```{r include=FALSE}
    fit_red <- lm(Esteem_PC1 ~ Education05 + Income87 + Inewspaper + Intelligence + Ilibrary + MotherEd,
                  data = temp)

    fit_full <- lm(Esteem_PC1 ~ Education05 + Income87 + Inewspaper + Intelligence + Ilibrary + MotherEd + Job05,
                  data = temp)

    anova(fit_red, fit_full)
    ```
    By comparing models with and without Job05, we identify an F-statistic of 2.21 and a corresponding p-value of 0.00017. Since the p-value < 0.05, we can conclude that Job05 adds significant explanatory power to the original model. As such, we will include Job05 in our model.

    ```{r include=FALSE}
    summary(fit_full)
    ```

    Thus, our final model is: Esteem_PC1 = −1.86 + 0.0939(Education05) + 0.0000169(Income87) + 0.256(Inewspaper) + 0.125(Intelligence) + 0.138(Ilibrary) + 0.0234(MotherEd) + Job05 where Job05 represents all of the different categories of job and the relevant effect on esteem for each of these. This has an adjusted r-squared of 0.131, meaning 13.1% of variation in Esteem_PC1 can be explained by variation in the independent variables, accounting for the number of independent variables. Furthermore, we calculate an F-statistic of 11.2 and a corresponding p-value less than 2x10^-16, indicating that the overall model is significant in explaining variation in the dependent variable. Finally, we calculate a residual standard error of 2.02, indicating that on average, datapoints are 2.02 units away from the regression line.

    Next, we will examin our assumptions of lineariy, normality and homoskedasticity.

    ```{r}
    qqnorm(residuals(fit_full))
    qqline(residuals(fit_full), col = "red")
    ```
```{r}
plot(fit_full, which = 1)
```
    
    To test the normality assumption, we can look at the QQ-Plot. The points lie very close to the diagonal line, even if there are slight curves at the lower and upper tails, however, we can assume that the normality assumption is met. For linearity, we are looking for an even, random distribution of points above and below zero. There is an even distribution of points before fitted value -1, however, after this, there is a clear, linear decrease in points converging at zero as the fitted values get more positive. As such, we cannot conclude that the linearlity assumption holds. Finally, looking at the vertical spread of points in the Residuals vs. Fitted plot, we do not see an even vertical spread of points, because it seems to be converging at zero as the fitted values get more positive. Consequently, this assumptions does not hold.
    
    Thus, the normality assumption holds, but the linearity and homoskedasticity assumptions do not hold.

    Looking at the final model, we can conclude that the variables that most affect one's self-esteem are Education05; Income87; Inewspaper; Intelligence; Ilibrary; MotherEd; and Job05. Thus, holding all other independent variables constant, for every increase in one unit of:
      - Education05, Self-Esteem will increase by 0.0939 on average.
      - Income87, Self-Esteem will increase 0.0000169 on average.
      - Inewspaper, Self-Esteem will increase 0.256 on average.
      - Intelligence, Self-Esteem will increase 0.125 on average.
      - Ilibrary, Self-Esteem will increase 0.138 on average.
      - MotherEd, Self-Esteem will increase 0.0234 on average.
      - Job05, where Self-Esteem will be impacted on a specific job-by-job basis.
    
    Similarly, the jobs with the greatest impact on Self-Esteem are:
      - Transportation and Material Moving Workers, Self-Esteem will decrease by 0.756 on average.
      - Construction Trade and Extraction Workers, Self-Esteem will decrease by 0.534 on average.
      - Entertainment Attendants and Related Workers, Self-Esteem will decrease by 1.76 on average.
      - Food Preparation and Serving Related Occupations, Self-Esteem will decrease by 0.871 on average.
      - Cleaning and Building Service Occupations, Self-Esteem will decrease by 0.823 on average.
      - Health Care Technical and Support Occupations, Self-Esteem will decrease by 0.842 on average.
      - Engineers, Architects, Surveyers, Engineering and Related Technicians, Self-Esteem will decrease by 0.699 on average.
      - Physical Scientists, Self-Esteem will decrease by 2.07 on average.
      - Executive, Administrative and Managerial Occupations, Self-Esteem will decrease by 0.649 on average.
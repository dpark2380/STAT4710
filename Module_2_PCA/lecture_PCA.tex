% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Principal Component Analysis},
  pdfauthor={Modern Data Mining},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Principal Component Analysis}
\author{Modern Data Mining}
\date{}

\begin{document}
\maketitle

{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{4}
\tableofcontents
}
\tableofcontents

\section*{Objectives}\label{objectives}
\addcontentsline{toc}{section}{Objectives}

Massive data is easily available to us. How can we efficiently extract
important information from a large number of features or variables which
will possess the following nice properties?

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  \textbf{Dimension reduction/noise reduction}: They are ``close'' to
  the original variables but only with a few newly formed variables.
\item
  \textbf{Grouping variables/subjects efficiently}: They will reveal
  insightful grouping structures.
\item
  \textbf{Visualization}: We can display high dimensional data.
\end{enumerate}

Principal Component Analysis is a powerful method to extract low
dimension variables. One may search among all linear combinations of the
original variables and find a few of them to achieve the three goals
above. Each newly formed variable is called a Principal Component. PCA
is closely related to Singular Value Decomposition (SVD). Both PCA and
Singular Value Decomposition are successfully applied in many fields
such as face recognition, recommendation system, text mining, Gene array
analyses among others. PCA is unsupervised learning. There will be no
responses. It works well in clustering analyses. In addition, PCs can be
used as input in supervised learning as well.

In this lecture, we analyze ASVAB tests (Armed Services Vocational
Aptitude Battery) using PCA to see how different people are. In
addition, PCA on test scores also reveals difference between males and
females: while the total test scores are similar, females are strong in
intelligence and males demonstrate better dexterity skills.

We will also apply SVD to build a recommender system based on the
existing ratings over a large number of movies.

\subsection*{PCA: Principal Component
Analysis}\label{pca-principal-component-analysis}
\addcontentsline{toc}{subsection}{PCA: Principal Component Analysis}

\begin{itemize}
\tightlist
\item
  Dimension reduction

  \begin{itemize}
  \tightlist
  \item
    capture the main features
  \item
    reduce the noise hidden in the data
  \item
    visualization of large dimension
  \end{itemize}
\item
  PC's interpretations

  \begin{itemize}
  \tightlist
  \item
    The best low dimension of linear approximation to the data (or
    closest to the data)
  \item
    The direction of linear combination which has largest variance
  \item
    We may take a small number of PCs as a set of input to other
    analyses
  \end{itemize}
\end{itemize}

\subsection*{Outline}\label{outline}
\addcontentsline{toc}{subsection}{Outline}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Case Study: ASVAB tests (Armed Services Vocational Aptitude Battery)
\item
  PCA

  \begin{itemize}
  \tightlist
  \item
    PC scores
  \item
    PC loadings
  \item
    PVE: determine the number of PCs
  \item
    Biplot: display the data
  \item
    \texttt{prcomp()}: PCA function
  \end{itemize}
\item
  Appendices:

  \begin{itemize}
  \tightlist
  \item
    Appendix 1: formal definition of PCs
  \item
    Appendix 2: PCA and Eigen decomposition of Correlation matrix
  \item
    Appendix 3: PCs and SVD
  \item
    Appendix 4: Missing values/recommender system
  \end{itemize}
\item
  Data needed:
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \texttt{IQ.Full.csv}
\item
  \texttt{MovieLens.csv}
\end{itemize}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  R functions
\end{enumerate}

\begin{itemize}
\tightlist
\item
  \texttt{prcomp()}: PCA function
\item
  \texttt{svd()}: SVD function
\item
  \texttt{factoextra} package: visualization of PCA results
\item
  \texttt{softImpute} package: SVD with missing values
\end{itemize}

\section{Case Study: How people differ in
intelligence?}\label{case-study-how-people-differ-in-intelligence}

\subsection{Background about
ASVAB/AFQT}\label{background-about-asvabafqt}

\textbf{ASVAB} (\href{https://www.officialasvab.com/}{Armed Services
Vocational Aptitude Battery}) tests have been used as a screening test
for those who want to join the army or other jobs. It helps to determine
which army jobs are appropriate for applicants. No preparation is
needed.

\textbf{ASVAB has the following components:}

\begin{itemize}
\item
  10 tests: \texttt{Science}, \texttt{Arith} (Arithmetic reasoning),
  \texttt{Word} (Word knowledge), \texttt{Parag} (Paragraph
  comprehension), \texttt{Numer} (Numerical operation), \texttt{Coding}
  (Coding speed), \texttt{Auto} (Automotive and Shop information),
  \texttt{Math} (Math knowledge), \texttt{Mechanic} (Mechanic
  Comprehension) and \texttt{Elec} (Electronic information).
\item
  \href{https://afqttest.com/}{AFQT} (Armed Forces Qualifying Test) is a
  combination of Word, Parag, Math and Arith.
\item
  Based on AFQT, one may qualify for service branch requirement: Air
  Force: 36, Army: 31, Marines: 32, Coast Guard: 40, Navy: 35 (out of
  100 which is the max!)
\end{itemize}

\textbf{Our goal}:

\begin{itemize}
\item
  How can we summarize the set of tests and grab main information about
  each one's intelligence/abilities efficiently?
\item
  How AFQT is formed?
\end{itemize}

\textbf{NLSY79 study and data:}

Data \texttt{IQ.Full.csv} is a subset of individuals from the 1979
National Longitudinal Study of Youth (NLSY79) survey who were
re-interviewed in 2006. Information on family, personal demographic such
as gender, race and education level, plus a set of ASVAB (Armed Services
Vocational Aptitude Battery) test scores, taken in 1981, are available.
In addition, a set of self-evaluated self-esteem scores and income in
2005 are also included in this dataset. The data is very interesting on
its own.

\textbf{Note:} One of the original study goals is to see how
intelligence relates to one's future successes measured by income in
2005 and self-esteem scores.

\textbf{Newer ASVAB data:} Do you have any more recent data related to
ASVAB?

(\textbf{\texttt{NLSY79.csv}} contains more information and it will be
used later in the course.)

\section{Data Prep and EDA}\label{data-prep-and-eda}

\textbf{Get a quick look at the data}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.full }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}\StringTok{"IQ.Full.csv"}\NormalTok{)  }
\FunctionTok{dim}\NormalTok{(data.full) }\CommentTok{\#str(data.full), summary(data.full)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2584   32
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(data.full)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1] "Subject"        "Imagazine"      "Inewspaper"     "Ilibrary"      
##  [5] "MotherEd"       "FatherEd"       "FamilyIncome78" "Race"          
##  [9] "Gender"         "Educ"           "Science"        "Arith"         
## [13] "Word"           "Parag"          "Numer"          "Coding"        
## [17] "Auto"           "Math"           "Mechanic"       "Elec"          
## [21] "AFQT"           "Income2005"     "Esteem1"        "Esteem2"       
## [25] "Esteem3"        "Esteem4"        "Esteem5"        "Esteem6"       
## [29] "Esteem7"        "Esteem8"        "Esteem9"        "Esteem10"
\end{verbatim}

There are 32 variables with 2,584 subjects/people. Many variables are
coded as numeric but categorical by nature. For example,
\texttt{Imagazine}, \texttt{Inewspaper} and \texttt{Ilibrary} have
\texttt{yes}, \texttt{no} as values. There seems to be no missing
values.

Our focus lies on analyzing \texttt{ASVAB} scores. We defer interesting
EDA's.

\section{\texorpdfstring{AFQT tests: \texttt{Word}, \texttt{Math},
\texttt{Parag} and
\texttt{Arith}}{AFQT tests: Word, Math, Parag and Arith}}\label{afqt-tests-word-math-parag-and-arith}

As one important summary of the \texttt{ASVAB} scores, AFQT scores
combining with \texttt{Word}, \texttt{Math}, \texttt{Parag} and
\texttt{Arith} are also reported for each test taker.

\textbf{Question:}

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  How best can we \textbf{capture the performance} using one or two
  scores based on the four tests?
\item
  Can we separate people with good language skills or math skills?
\item
  How is AFQT calculated? Is it merely the total scores of the four
  tests?
\end{enumerate}

\textbf{Note:}

This is similar to the creation of SP500, a weighted index based on 500
stocks.

\textbf{A subset:} For simplicity we take a subset of 50 subjects and
only include the \texttt{AFQT} and the four tests associated with it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cols }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Subject"}\NormalTok{, }\StringTok{"Word"}\NormalTok{, }\StringTok{"Parag"}\NormalTok{, }\StringTok{"Math"}\NormalTok{, }\StringTok{"Arith"}\NormalTok{, }\StringTok{"AFQT"}\NormalTok{, }\StringTok{"Gender"}\NormalTok{)}
\NormalTok{AFQT.full }\OtherTok{\textless{}{-}}\NormalTok{ data.full[, cols]}
\CommentTok{\# full data set of the AFQT scores, AFQT and Gender}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{10}\NormalTok{) }\CommentTok{\# make sure the same subset is generated each time   }
\NormalTok{AFQT.sub }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.full[}\FunctionTok{sample}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(data.full), }\DecValTok{50}\NormalTok{, }\AttributeTok{replace=}\ConstantTok{FALSE}\NormalTok{), cols] }
\DocumentationTok{\#\# dplyr way:}
\CommentTok{\# set.seed(10)}
\CommentTok{\# AFQT.sub \textless{}{-} data.full \%\textgreater{}\% sample\_n(50, replace = F) \%\textgreater{}\%}
\CommentTok{\#   select(Subject,Word, Parag, Math, Arith, AFQT)}
\FunctionTok{str}\NormalTok{(AFQT.sub) }\CommentTok{\# hist(AFQT.sub$Word)}
\NormalTok{summary.AFQT }\OtherTok{\textless{}{-}} \FunctionTok{skim}\NormalTok{(AFQT.sub)}
\FunctionTok{names}\NormalTok{(summary.AFQT) }\CommentTok{\# see skim\textquotesingle{}s output}
\NormalTok{summary.AFQT }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(skim\_variable, numeric.mean, numeric.sd, numeric.hist)}
\CommentTok{\# only need mean, sd and hist}
\end{Highlighting}
\end{Shaded}

The four tests have different means and different standard deviations.

\textbf{Is \texttt{AFQT} the sum of the four test scores?} Not
really!But they are highly correlated.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\AttributeTok{x =}\NormalTok{ AFQT.sub}\SpecialCharTok{$}\NormalTok{AFQT, }
     \AttributeTok{y =} \FunctionTok{rowSums}\NormalTok{(AFQT.sub }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Word, Parag, Math, Arith)),}
     \AttributeTok{xlab =} \StringTok{"AFQT"}\NormalTok{, }
     \AttributeTok{ylab=}\StringTok{"Total of the four tests"}\NormalTok{, }
     \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{))}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{a =} \DecValTok{0}\NormalTok{, }\AttributeTok{b =}\DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{lwd =}\DecValTok{3}\NormalTok{) }\CommentTok{\# a=intercept, b=slope}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h =} \FunctionTok{mean}\NormalTok{(AFQT.sub}\SpecialCharTok{$}\NormalTok{AFQT), }\AttributeTok{col =} \StringTok{"red"}\NormalTok{, }\AttributeTok{lwd =}\DecValTok{3}\NormalTok{) }\CommentTok{\# a horizontal line of y= sample mean}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-1-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#cor(AFQT.sub$AFQT, rowSums(AFQT.sub \%\textgreater{}\% select(Word, Parag, Math, Arith)))}
\end{Highlighting}
\end{Shaded}

For simplicity we give names for each person in the subset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{rownames}\NormalTok{(AFQT.sub)  }\CommentTok{\# label for each person}
\FunctionTok{rownames}\NormalTok{(AFQT.sub) }\OtherTok{\textless{}{-}}  \FunctionTok{paste}\NormalTok{(}\StringTok{"p"}\NormalTok{, }\FunctionTok{seq}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\FunctionTok{nrow}\NormalTok{(AFQT.sub)), }\AttributeTok{sep=}\StringTok{""}\NormalTok{)  }
\CommentTok{\# reassign everyone\textquotesingle{}s labels to be shorter.}
\FunctionTok{rownames}\NormalTok{(AFQT.sub)}
\end{Highlighting}
\end{Shaded}

\subsection{Motivations/Interpretations of
PCA}\label{motivationsinterpretations-of-pca}

\subsubsection{PCA for only two tests}\label{pca-for-only-two-tests}

Let us focus on \texttt{Word} and \texttt{Parag} first by starting with
a scatter plot of the original \texttt{Parag} and \texttt{Word} scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parag.word  }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(AFQT.sub[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Parag"}\NormalTok{, }\StringTok{"Word"}\NormalTok{)])}

\NormalTok{mu\_parag }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(parag.word}\SpecialCharTok{$}\NormalTok{Parag, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{sd\_parag }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(parag.word}\SpecialCharTok{$}\NormalTok{Parag,   }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{mu\_word  }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(parag.word}\SpecialCharTok{$}\NormalTok{Word,  }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{sd\_word  }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(parag.word}\SpecialCharTok{$}\NormalTok{Word,    }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{parag.word }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Parag, }\AttributeTok{y =}\NormalTok{ Word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ mu\_parag) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =}\NormalTok{ mu\_word) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_fixed}\NormalTok{(}
    \AttributeTok{ratio =} \DecValTok{1}\NormalTok{,}
    \AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(mu\_parag }\SpecialCharTok{{-}} \DecValTok{3}\SpecialCharTok{*}\NormalTok{sd\_parag, mu\_parag }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{sd\_parag),}
    \AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(mu\_word  }\SpecialCharTok{{-}} \DecValTok{3}\SpecialCharTok{*}\NormalTok{sd\_word,  mu\_word  }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{sd\_word)}
\NormalTok{  ) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Original \textasciigrave{}Parag\textasciigrave{} vs Word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/scatter plot of original scaled data-1} \end{center}

How can we grab some main information about each person's language
skills using one score or two scores based on the two tests
\texttt{Word} and \texttt{Parag}?

We want to use one aggregated score or weighted sum of two scores with
the following desirable properties:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The new score is weighted sum of \texttt{Word} and \texttt{Parag},
  i.e., a linear combination of the two. We denote this by
  \(Z_1 = \phi_{11} \text{Word} +\phi_{21} \text{Parag}\)
\item
  The two scores \texttt{Word} and \texttt{Parag} should be ``close'' to
  the newly formed one score \(Z_1\).
\end{enumerate}

In other words, we are looking for a line, with a direction by
\((\phi_{11}, \phi_{21})\), going through the cloud of the scatter plot
of \texttt{Word} vs.~\texttt{Parag} with minimum overall perpendicular
distance. The projection of each point (Parag, Word), i.e., the weighted
sum of the two scores \(Z_1\) is called a Principal Component or a PC.

In the following sections, we give both informal and formal definitions
of PCs and also show how to find the \texttt{weights} or
\texttt{loadings}, \((\phi_{11}, \phi_{21})\) and the PC scores
\(Z_1 = \phi_{11} \text{Word} +\phi_{21} \text{Parag}\). We also define
other PC's.

\subsubsection{Geometric
interpretations}\label{geometric-interpretations}

The crux of PCA can be captured simply by a plot. Focus on the plots in
this section. (Codes are hidden on propose. \textbf{YOU DON'T NEED TO
KNOW THE CODES used to produce plots in this section!!!!})

To demonstrate what are PCs and the geometric properties of PCAs, let us
look at the scatter plots with centered \texttt{Word} and \texttt{Parag}
scores. i.e., we subtract \texttt{Word} and \texttt{Parag} by its mean,
respectively. We call this process centering the data. Positive centered
score implies the raw score is above the mean and below the mean if it
is negative.

In the following R-chunk, we first center the two scores then make a
scatter plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parag.word.centered }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.sub }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Word, Parag) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word\_centered =} \StringTok{\textasciigrave{}}\AttributeTok{Word}\StringTok{\textasciigrave{}} \SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Word),}
         \AttributeTok{parag\_centered =} \StringTok{\textasciigrave{}}\AttributeTok{Parag}\StringTok{\textasciigrave{}} \SpecialCharTok{{-}} \FunctionTok{mean}\NormalTok{(Parag))}
\CommentTok{\# Making word and parag by centering each score.}

\CommentTok{\# or use scale() to center the data}
\NormalTok{parag.word.centered }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(AFQT.sub[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Parag"}\NormalTok{, }\StringTok{"Word"}\NormalTok{)], }
                                           \AttributeTok{center =}\NormalTok{ T, }\AttributeTok{scale =}\NormalTok{ F)}
\NormalTok{parag.word.centered }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(parag.word.centered)}
\CommentTok{\# make centered data as a data frame}
\end{Highlighting}
\end{Shaded}

Notice the original and centered data only differ by the mean values
while keep the same standard deviations.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{sapply}\NormalTok{(parag.word.centered,mean), }\DecValTok{3}\NormalTok{) }\CommentTok{\# col mean with 3 decimals}
\FunctionTok{sapply}\NormalTok{(AFQT.sub }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Word, Parag), mean) }\CommentTok{\# col mean}
\FunctionTok{sapply}\NormalTok{(parag.word.centered,sd) }\CommentTok{\#col sd}
\FunctionTok{sapply}\NormalTok{(AFQT.sub }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{( Parag, Word), sd) }\CommentTok{\# col sd }
\end{Highlighting}
\end{Shaded}

Look at the scatter plot of centered \texttt{Word} and \texttt{Parag}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parag.word.centered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Parag, }\AttributeTok{y =}\NormalTok{ Word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_fixed}\NormalTok{(}\AttributeTok{ratio =} \DecValTok{1}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{10}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{20}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Centered \textasciigrave{}Parag\textasciigrave{} vs Word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-2-1} \end{center}

The following scatter plot of \texttt{Word} vs.~\texttt{Parag}
illustrates what is a desirable linear line we are looking for:
\textbf{the total squared perpendicular distance from each point to the
line should be minimized.}

\textbf{No need to go through the chunks below. Focus on the plot.}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-3-1} \end{center}

PC1, first principal component: the linear combination of the two scores
which minimizes the total squared perpendicular distance. That line is
described by (\texttt{procom} function is used to produce all relevant
quantities.)

\begin{itemize}
\tightlist
\item
  PC1 loadings: \textbf{\((0.3, 0.96)\)} which describes the direction
  of the line.
\item
  PC1 scores: the projection score
  \textbf{\(0.3\times \text{Parag}\_{\text{centered}} + 0.96\times \text{Word}\_{\text{centered}}\)}.
\end{itemize}

As an example, for the person with \(\texttt{Parag\_centered} = 4.58\)
and \(\texttt{Word\_centered} = 5.08\), the PC score is
\(.3\times 4.58+.96\times 5.08 = 6.25\) (It should be \(6.21\) as marked
in the plot above due to rounding error.)

\textbf{How much information lost using PC1?}

Instead of using \texttt{Word} and \texttt{Parag} we only use PC1. We
will lose on average mean sum of squared distances.

\subsubsection{Two defintions or interpretations of
PCA}\label{two-defintions-or-interpretations-of-pca}

The above PC scores may have one problem: the two tests have different
spread or standard deviation. Often we may want to find PCs among
totally different variables with different units. In this case, it is a
good idea to center and scale the data, by subtracting the mean and
dividing the standard deviation for each test first, before performing
PCA. This can be done using \texttt{scale()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{parag.word.scaled.centered  }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{scale}\NormalTok{(AFQT.sub[, }\FunctionTok{c}\NormalTok{(}\StringTok{"Parag"}\NormalTok{, }\StringTok{"Word"}\NormalTok{)], }
                                           \AttributeTok{center =}\NormalTok{ T, }\AttributeTok{scale =}\NormalTok{ T))}
\NormalTok{parag.word.scaled.centered }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ Parag, }\AttributeTok{y =}\NormalTok{ Word)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_point}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{coord\_fixed}\NormalTok{(}\AttributeTok{ratio =} \DecValTok{1}\NormalTok{, }\AttributeTok{xlim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{), }\AttributeTok{ylim =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{3}\NormalTok{, }\DecValTok{3}\NormalTok{)) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"Centered and scaled \textasciigrave{}Parag\textasciigrave{} vs Word"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-4-1} \end{center}

The following plot illustrates the relationship among three metrics:

\begin{itemize}
\tightlist
\item
  Total sum of squares
\item
  Sum of squared errors
\item
  Variance of PC1
\end{itemize}

Skip the codes but concentrating on the plot please:

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-5-1} \end{center}

In the above plot we want to demonstrate the following beautiful
geometric interpretation of PCA.

\textbf{Fact 1: A line which minimizes the total squared distance must
go through the origin (or sample means)}

\textbf{Fact 2:} By the Pythagorean theorem, for any point:

\[\color{red}{\text{PC score}^2} + \color{blue}{\text{Perpendicular distance}^2} = \color{green}{\text{Distance to origin}^2}\]

Adding all the terms for each point, we have the following striking
relationship:

\[\color{red}{\frac{1}{n-1} \text{Sum}(\text{PC score}^2 )} + \color{blue}{\frac{1}{n-1} \text{Sum}(\text{Perpendicular distance}^2)} = \color{green}{\frac{1}{n-1} \text{Sum}(\text{Distance to origin}^2)}\]

Notice:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\color{green}{\text{Sum}(\text{Distance to origin}^2)}\) never
  changes.
\item
  \(\color{red}{\frac{1}{n-1} \text{Sum}(\text{PC score}^2 ) = Var(\text{PC scores})}\)
\item
  \(\color{blue}{\frac{1}{n-1} \text{Sum}(\text{Perpendicular distance}^2) = \text{Mean squred errors}}\)
\end{enumerate}

Hence, maximizing the variance of PC scores is equivalent to minimizing
the mean squared error (perpendicular distances). Note that minimizing
the mean \emph{perpendicular} distances here is different from simple
linear regression that minimizes the \emph{vertical} distances between
the linear line and points. Now we are ready to reveal two equivalent
definitions of PCs:

\textbf{Definition 1:} The linear combination which minimizes the total
squared perpendicular distance

\textbf{Definition 2:} The linear combination with maximum variance or
with largest spread or formally, we are looking for a pair of weights
\(\phi_{11}\) and \(\phi_{21}\) such that

\[Z_1=\phi_{11}X_1 + \phi_{21}X_2 \] such that
\[\max_{\phi_{11}^2+ \phi_{21}^2 =1}\mathrm{Var}(Z_1 = \phi_{11}X_1 + \phi_{21}X_2)\]

\subsubsection{An animation to find the optimal weights
(loadings)}\label{an-animation-to-find-the-optimal-weights-loadings}

The following chuck illustrates when the linear combination of different
weights or the line has a different slope. The relationship between the
three sums changes exactly as we have shown above.

As sum of squared errors increases, variance of the line decreases while
both sums never change. Of course when the line minimizes the sum of
squared errors it also maximizes the variance of the PC scores which
gives us the First Principal Component!

We used \texttt{shiny} to make this illustration. When execute the
following chunk, \textbf{a separate window will pop out.} By changing
the slope of the line, you will see how the projection of points changes
and how the squared distance and variance change. Compare with the red
PC component line. (Remember to kill the graph once you are done;
otherwise the following chunk will be running all the time.)

\subsubsection{Other Principal
Components}\label{other-principal-components}

Once we find the leading principal component, we can look for the second
linear combination of the \texttt{Word} and \texttt{Parag} such that the
line goes through the data points with minimum squared distance or
largest variance but with one constrain - the line needs to be
perpendicular to the first principal component. We call that line,
second Principal component.

Or mathematically we are looking for another linear transformation
\(Z_2\) of \(X_1=\texttt{Word}\), \(X_2=\texttt{Parag}\) to have the max
variance of \(Z_2\) and \(Z_1\) is orthogonal to \(Z_2\).

This is same as finding \((\phi_{12}, \phi_{22})\) such that
\(Z_2 = \phi_{12} X_1 + \phi_{22 }X_2\) and

\[\max_{\phi_{12}^2+ \phi_{22}^2 =1}\mathrm{Var}(Z_2)\]

and two sets of loadings are perpendicular: or
\((\phi_{11}, \phi_{21})\) and \((\phi_{12}, \phi_{22})\) are
orthogonal.

Remark: By the definition we know the variance of PC1 is larger than
that of PC2.

The following plot shows how to find the second principal component.

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-6-1} \end{center}

If we only use PC1, we will lose some information contained in two
scores. The error is described by the sum of squared error or the
variance of PC1.

If we use both PC1 and PC2 we do not lose any information at all. In
other words, the variance of PC1 plus variance of PC2 is exactly
variance of \texttt{Word} + variance of \texttt{Parag}!

Remark: We can have maximum two PCs when there are only two variables.

\section{Principal Component of AFQT
tests}\label{principal-component-of-afqt-tests}

AFQT contains four tests. Our goal is to use less number of variables to
capture the information contained in 4 scores. Analogous to two
variables, we can define PC's for 4 variables. The leading PC's would be
a linear combination of 4 scores which maximize the variance of the
linear combination. We have postpone the formal definitions to the
appendices.

There will be no more than 4 PCs since there are only 4 variables. Each
PC will be controlled by the loadings or the weights to each variable.
All 4 sets of loadings are orthogonal with unit 1. All 4 PCs are also
orthogonal or uncorrelated with decreasing variances.

We hope using a few principal components to capture the structure among
all variables. Often the clustering information may appear in PC
coordinates. If we are lucky enough we may discover clear
interpretations of each PC in terms of the original scores. In general
we may lose interpretation from each score.

\textbf{Question:}

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  What does each PC score mean?
\item
  How many PCs should be used?
\item
  What interesting facts can be revealed by PCs?
\end{enumerate}

\subsection{Find PCs and Loadings}\label{find-pcs-and-loadings}

\texttt{prcomp()} is the main function used to give us all the loadings
and PCs and variances of each PC. You will find simple, beautiful
mathematics how PCA is done through eigen decomposition and SVD
(Singular Value Decomposition) in Appendix.

To conduct PCA:

Step I: To find sensible PCs, it is recommended to

\begin{itemize}
\tightlist
\item
  center each variable by subtracting its mean
\item
  scale each variable by dividing its sd (rather complex on this issue)
\item
  \texttt{prcomp()} has an option to scale or we could use
  \texttt{scale()} explicitly
\end{itemize}

Step II: Run \texttt{prcomp()}

\begin{itemize}
\tightlist
\item
  Output all the loadings: one set for each PC
\item
  Obtain all PC scores
\item
  Report the variances for each PC and for each original variable
\end{itemize}

We next perform PCA for four variables \texttt{Word}, \texttt{Parag},
\texttt{Math}, \texttt{Arith}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.AFQT }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.sub }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Word, Parag, Math, Arith) }
\CommentTok{\# summary(data.AFQT) \# no sd\textquotesingle{}s }
\CommentTok{\# skim(data.AFQT) \# provides mean, sd and more}
\CommentTok{\# skim(data.AFQT) \%\textgreater{}\% select(skim\_variable, numeric.mean, numeric.sd)}
\NormalTok{pc}\FloatTok{.4} \OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(data.AFQT, }\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{, }\AttributeTok{center=}\NormalTok{T)  }\CommentTok{\# by default, center=True but scale=FALSE!!!}
\FunctionTok{names}\NormalTok{(pc}\FloatTok{.4}\NormalTok{) }\CommentTok{\#check output }
\CommentTok{\# rotation: loadings pc.4$rotat}
\CommentTok{\# x: PC scores}
\CommentTok{\# sdev: standard dev of the PC scores (pc.4$sdev)}
\CommentTok{\# pc.4$center \# sample mean of the original x\textquotesingle{}s}
\CommentTok{\# pc.4$scale   \# sd of the original x\textquotesingle{}s}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sdev"     "rotation" "center"   "scale"    "x"
\end{verbatim}

\texttt{prcomp()} outputs the following:

\begin{itemize}
\tightlist
\item
  \texttt{\$rotation}: loadings
\item
  \texttt{\$x}: PC scores
\item
  \texttt{\$sdev}: standard deviations of the four PC's
\item
  \texttt{\$center}: means of the four tests
\item
  \texttt{\$scale}: standard deviatoins of the four tests
\end{itemize}

\textbf{Loadings}

Each loadings give us a set of four numbers which determines the
direction of each \textbf{line}. Let us take a look at the leading PC's
loadings:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pc.}\FloatTok{4.}\NormalTok{loading }\OtherTok{\textless{}{-}}\NormalTok{ pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{rotation   }\CommentTok{\#pc.4$x (pc.4$sd)\^{}2}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(pc.}\FloatTok{4.}\NormalTok{loading)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
& PC1 & PC2 & PC3 & PC4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Word & 0.509 & 0.442 & -0.365 & -0.642 \\
Parag & 0.500 & 0.546 & 0.308 & 0.598 \\
Math & 0.496 & -0.483 & 0.652 & -0.310 \\
Arith & 0.494 & -0.523 & -0.589 & 0.368 \\
\end{longtable}

\textbf{Remark:}

\begin{itemize}
\tightlist
\item
  Loadings are unique up to sign. For example PC1 loading can be
  \((.51, .50, .5, .5)\) or \((-.51, -.50, -.5, -.5)\). Why so???
\item
  The magnitude of loadings tells us how much each variable contributes
  to the PCs.
\end{itemize}

\textbf{PCs}

We can get PCs by taking the linear combination of loadings and
variables as: \begin{align*}
  \texttt{PC1} &= 0.509 \times \texttt{Word\_centered\_scaled} + 0.5 \times \texttt{Parag\_centered\_scaled} \\
  &~~~~+ 0.496 \times \texttt{Math\_centered\_scaled} + 0.494 \times \texttt{Arith\_centered\_scaled} \\
  \texttt{PC2} &= (0.442) \times \texttt{Word\_centered\_scaled} + (0.546) \times \texttt{Parag\_centered\_scaled} \\
  &~~~~+ -0.483 \times \texttt{Math\_centered\_scaled} + -0.523 \times \texttt{Arith\_centered\_scaled}
\end{align*}

We will continue to get PC3 and PC4.

All the PCs are computed. Each person will have four PC scores. Let us
take PCs for the first 5 people

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, ])}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}
\toprule\noalign{}
& PC1 & PC2 & PC3 & PC4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
p1 & -0.920 & 0.044 & 0.923 & -0.611 \\
p2 & 0.611 & -1.171 & -0.585 & -0.004 \\
p3 & -0.312 & 0.697 & 0.100 & -0.683 \\
p4 & 3.069 & -0.049 & 0.314 & 0.105 \\
p5 & 2.818 & -0.440 & 0.219 & 0.112 \\
\end{longtable}

\textbf{Interpretations of loadings and PCs}

Loadings determine contribution of each variable to the PCs. Loadings
are also proportional to the correlations between each PC and variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pc.}\FloatTok{4.}\NormalTok{loading}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         PC1    PC2    PC3    PC4
## Word  0.509  0.442 -0.365 -0.642
## Parag 0.500  0.546  0.308  0.598
## Math  0.496 -0.483  0.652 -0.310
## Arith 0.494 -0.523 -0.589  0.368
\end{verbatim}

\textbf{PC1:} since the four loadings are approximately the same around
.5 so PC1 is proportional to the total of the four scores. In other
words: \begin{align*}
\texttt{PC1} &= .5 \times (\texttt{Word\_centered\_scaled} +  \texttt{Parag\_centered\_scaled} \\
&~~~~~~~~~~~~+ \texttt{Math\_centered\_scaled} + \texttt{Arith\_centered\_scaled})
\end{align*}

Higher PC1 \(\Longrightarrow\) Higher weighted total score.

\textbf{PC2:} \begin{align*}
\texttt{PC2} &= .5 \times (\texttt{Word\_centered\_scaled} +  \texttt{Parag\_centered\_scaled}) \\
&~~~~-.5 \times (\texttt{Math\_centered\_scaled} + \texttt{Arith\_centered\_scaled})
\end{align*}

\begin{itemize}
\tightlist
\item
  Approximately proportional to the difference between to sum of
  \texttt{Math}/\texttt{Arith} and sum of \texttt{Word} and
  \texttt{Parag}
\item
  If total scores are comparable, higher PC2 implies strong math talent
  while lower PC2 implies superior language ability
\end{itemize}

Combine centered and scaled \texttt{Word}, \texttt{Parag},
\texttt{Math}, \texttt{Arith} with \texttt{PC1}, \texttt{PC2},
\texttt{PC3}, \texttt{PC4}. We list a few people's scores and PCs. Can
you calculate the PCs from the \texttt{Word}, \texttt{Parag},
\texttt{Math}, \texttt{Arith} using the loadings?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AFQT.PC.Scores }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(}\FunctionTok{scale}\NormalTok{(data.AFQT, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{), pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x)}
   \FunctionTok{arrange}\NormalTok{(}\FunctionTok{as.data.frame}\NormalTok{(AFQT.PC.Scores), }\FunctionTok{desc}\NormalTok{(PC1)) }\SpecialCharTok{\%\textgreater{}\%}
     \FunctionTok{head}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      Word Parag Math Arith  PC1     PC2    PC3     PC4
## p30 1.360 1.302 1.89  1.67 3.11 -0.4783  0.153 -0.0642
## p4  1.360 1.666 1.72  1.40 3.07 -0.0492  0.314  0.1052
## p5  1.090 1.302 1.72  1.54 2.82 -0.4399  0.219  0.1121
## p21 0.955 0.575 2.07  1.81 2.70 -1.2121  0.109 -0.2426
## p49 0.281 1.302 1.72  1.26 2.27 -0.6519  0.678  0.5290
## p44 0.820 0.575 1.19  1.54 2.06 -0.7020 -0.251  0.0136
\end{verbatim}

\textbf{Loadings and correlations between PC and each scores:}

Loadings account for weights of each variable in the PC. They are in
fact proportional to the correlation between PC to each score with
\texttt{sd(PC)} as a factor. In other words,
\[ \texttt{Corr(PC1, data.AFQT.scale)} = \texttt{sd(PC1)}\times \texttt{PC1\_loadings} \]

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\NormalTok{], }\FunctionTok{scale}\NormalTok{(data.AFQT, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{))[}\DecValTok{1}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Word Parag  Math Arith 
## 0.891 0.874 0.868 0.865
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\NormalTok{]) }\SpecialCharTok{*}\NormalTok{ pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{rot[,}\DecValTok{1}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  Word Parag  Math Arith 
## 0.891 0.874 0.868 0.865
\end{verbatim}

\subsection{Scatter plot of PCs}\label{scatter-plot-of-pcs}

Often a scatter plot of PCs may reveal interesting information. For
example, we know PC1 and PC2 have clear interpretaion, by plotting PC2
vs.~PC1, we can locate people with different strength.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.data.frame}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{()}\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"PC2 vs. PC1 for AFQT"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-10-1} \end{center}

People on the far right are high in total scores. The first quadrant
contains people with strong math skills while one in the forth quadrant
good in language skills.

\subsection{Properties of PCs and
Loadings}\label{properties-of-pcs-and-loadings}

\textbf{All loadings are perpendicular and with unit 1}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{t}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{rotation) }\SpecialCharTok{\%*\%}\NormalTok{ pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{rotation) }\CommentTok{\# to check the loadings are unit 1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     PC1 PC2 PC3 PC4
## PC1   1   0   0   0
## PC2   0   1   0   0
## PC3   0   0   1   0
## PC4   0   0   0   1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# or}
\FunctionTok{colSums}\NormalTok{((pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{rotation)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## PC1 PC2 PC3 PC4 
##   1   1   1   1
\end{verbatim}

\textbf{\texttt{var(PC1)} \(>\) \texttt{var(PC2)} \(>\) \ldots{}} and
they add up to be 4. Why so?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{((pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{sdev)}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{)  }\CommentTok{\# Var(PC1), Var(PC2),...\#sum((pc.4$sdev)\^{}2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.06 0.49 0.27 0.18
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# knitr::kable(summary(pc.4)$importance)   }
\end{Highlighting}
\end{Shaded}

Notice var(PC1) is much larger than the rest of the variances. PC1
captures large amount of variability in the data.

\textbf{All 4 PC scores are uncorrelated}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{round}\NormalTok{(}\FunctionTok{cov}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x), }\DecValTok{4}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      PC1   PC2   PC3  PC4
## PC1 3.06 0.000 0.000 0.00
## PC2 0.00 0.488 0.000 0.00
## PC3 0.00 0.000 0.271 0.00
## PC4 0.00 0.000 0.000 0.18
\end{verbatim}

From the following pair-wise plots we see the variability of each PC in
a decreasing scale.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pairs}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{), }\AttributeTok{col=}\FunctionTok{rainbow}\NormalTok{(}\DecValTok{6}\NormalTok{), }\AttributeTok{pch=}\DecValTok{16}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-14-1} \end{center}

\subsection{Proportion of variance explained
(PVE)}\label{proportion-of-variance-explained-pve}

One goal of principal component is to find as few as many PCs which have
as large variances as possible. How many PCs are informative? We
introduce the measurement of proportion of variance explained (PVE) as

\[ \mathrm{PVE} = \mathrm{Var}(\mathrm{PC}) \,/\, \text{Total Variances} \]

We can calculate the PVE or get proportion of variance from the output

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pc}\FloatTok{.4}\NormalTok{)}\SpecialCharTok{$}\NormalTok{importance  }\CommentTok{\#notice it is from summary()  names(summary(pc.4))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                          PC1   PC2    PC3    PC4
## Standard deviation     1.750 0.699 0.5208 0.4238
## Proportion of Variance 0.765 0.122 0.0678 0.0449
## Cumulative Proportion  0.765 0.887 0.9551 1.0000
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(pc}\FloatTok{.4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "sdev"     "rotation" "center"   "scale"    "x"
\end{verbatim}

The summary reports standard deviations, PVE and cumulative proportions.

For example, the leading principal component explains 0.765 of the total
variance.

We also see clearly that variance of PC1 is larger than that of PC2,
etc.

\textbf{Scree Plots}

A scree plot of PVE or Cumulative PVE can help us to see how much
variance is captured by each PC.

\textbf{Scree plot of variances:}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pc}\FloatTok{.4}\NormalTok{) }\CommentTok{\# variances of each pc}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-16-1} \end{center}

\textbf{How many PCs to use?}

We may look at the scree plot of PVEs and apply elbow rules: take the
number of PCs when there is a sharp drop in the scree plot.

Here is the scree plot of PVEs.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pc}\FloatTok{.4}\NormalTok{)}\SpecialCharTok{$}\NormalTok{importance[}\DecValTok{2}\NormalTok{, ],  }\CommentTok{\# PVE}
     \AttributeTok{ylab=}\StringTok{"PVE"}\NormalTok{,}
     \AttributeTok{xlab=}\StringTok{"Number of PCs"}\NormalTok{,}
     \AttributeTok{pch =} \DecValTok{16}\NormalTok{, }
     \AttributeTok{main=}\StringTok{"Scree Plot of PVE for AFQT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-17-1} \end{center}

\textbf{It indicates that two leading PCs should be enough} for certain
purposes.

Lastly we may look at the cumulative variance explained by each PC.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pc}\FloatTok{.4}\NormalTok{)}\SpecialCharTok{$}\NormalTok{importance[}\DecValTok{3}\NormalTok{, ], }\AttributeTok{pch=}\DecValTok{16}\NormalTok{,}
     \AttributeTok{ylab=}\StringTok{"Cumulative PVE"}\NormalTok{,}
     \AttributeTok{xlab=}\StringTok{"Number of PCs"}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Scree Plot of Cumulative PVE for AFQT"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-18-1} \end{center}

\subsection{Biplot}\label{biplot}

Visualize the PC scores together with the loadings of the original
variables. They also reveal correlation structures among all variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lim }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{4}\NormalTok{, .}\DecValTok{4}\NormalTok{) }
\FunctionTok{biplot}\NormalTok{(pc}\FloatTok{.4}\NormalTok{,        }\CommentTok{\# choices=c(1,3), }
       \AttributeTok{xlim=}\NormalTok{lim,}
       \AttributeTok{ylim=}\NormalTok{lim,}
       \AttributeTok{main=}\StringTok{"Biplot of the PCs"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{v=}\DecValTok{0}\NormalTok{, }\AttributeTok{h=}\DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-19-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# x{-}axis: PC1 (prop)}
\CommentTok{\# y{-}axis: PC2}
\CommentTok{\# top x{-}axis: prop to loadings for PC1}
\CommentTok{\# right y{-}axis: prop to loadings for PC2}
\CommentTok{\# using argument of choices = c(1,3), we can explore scatter plots of other PCs}
\end{Highlighting}
\end{Shaded}

Better biplot using factoextra package (factor analysis visualization
not a categorical variable)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fviz\_pca\_biplot}\NormalTok{(}
\NormalTok{  pc}\FloatTok{.4}\NormalTok{,}
  \AttributeTok{label =} \StringTok{"var"}\NormalTok{,}
  \AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{pointsize =} \FloatTok{1.2}\NormalTok{,}
  \AttributeTok{alpha.ind =} \FloatTok{0.4}\NormalTok{,}
  \AttributeTok{col.var =} \StringTok{"black"}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-20-1} \end{center}

The biplot indicates

\begin{itemize}
\tightlist
\item
  PC1 loadings are similar in magnitudes and with same signs
\item
  PC2 captures difference between total of \texttt{Math}, \texttt{Arith}
  and total of \texttt{Word} and \texttt{Parag}
\item
  \texttt{Math} and \texttt{Arith} are highly correlated, and so are
  \texttt{Word} and \texttt{Parag}!
\end{itemize}

\subsection{Gender effects?}\label{gender-effects}

Are there systematic differences among men and women? By various plots
of PCs we try to see any possible patterns.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.data.frame}\NormalTok{(pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ AFQT.sub}\SpecialCharTok{$}\NormalTok{Gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2)) }\SpecialCharTok{+}    \CommentTok{\# Try other PCs vs. PC1, any patterns?}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{gender))}\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"PC2 vs. PC1 for AFQT"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-21-1} \end{center}

We couldn't tell any relationship between Gender and AFQT.

\textbf{Lastly AFQT PCs and Gender:}

So far the PCA is done for a subset of 50 subjects. Finally we bring all
subjects and run PCA of over \texttt{AFQT} and \texttt{Gender}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pc.}\FloatTok{4.}\NormalTok{all }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.full }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Word, Parag, Arith, Math) }\SpecialCharTok{\%\textgreater{}\%}
     \FunctionTok{prcomp}\NormalTok{(}\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\#pc.4.all$rotation \# check the signs}
\CommentTok{\#pc.4.all$rotation}
\FunctionTok{as.data.frame}\NormalTok{(pc.}\FloatTok{4.}\NormalTok{all}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ AFQT.full}\SpecialCharTok{$}\NormalTok{Gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{PC1, }\AttributeTok{y=}\NormalTok{PC2)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color=}\NormalTok{gender))}\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"PC2 vs. PC1 for AFQT"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-22-1} \end{center}

Questions: - What are the PC1 and PC2 loadings? - What are the
interpretations of PC1 and PC2? - Do you see systematic difference
between men's and women's AFQT scores? - How do you summarize the
performance based on the above PCs plot?

\subsection{Summary}\label{summary}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  To capture the main features of AFQT four scores we could use two
  newly formed PC scores:

  \begin{itemize}
  \tightlist
  \item
    PC1: Total scores (weighted)
  \item
    PC2: Difference between \texttt{Math}+\texttt{Arith} and
    \texttt{Word}+\texttt{Parag}
  \end{itemize}
\item
  \textbf{What is AFQT score} reported? Might it be the PC1 of the four
  tests? Or is it similar to the total scores?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(AFQT.full}\SpecialCharTok{$}\NormalTok{AFQT, pc.}\FloatTok{4.}\NormalTok{all}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\NormalTok{]) }\CommentTok{\#plot(AFQT.full$AFQT, pc.4.all$x[, 1])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.966
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{total\_no\_weight }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.full }\SpecialCharTok{\%\textgreater{}\%}   \CommentTok{\# create total scores}
    \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total =}\NormalTok{ Word}\SpecialCharTok{+}\NormalTok{Parag}\SpecialCharTok{+}\NormalTok{Math}\SpecialCharTok{+}\NormalTok{Arith) }\SpecialCharTok{\%\textgreater{}\%}
    \FunctionTok{select}\NormalTok{(total)}
\FunctionTok{cor}\NormalTok{(AFQT.full}\SpecialCharTok{$}\NormalTok{AFQT, total\_no\_weight) }\CommentTok{\# data.frame(AFQT.full$AFQT, total\_no\_weight)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      total
## [1,] 0.964
\end{verbatim}

Final questions to ask: what happens if we run PCA without scaling?

PCA on AFQT without scaling:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pc.}\FloatTok{4.}\NormalTok{no.scale }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(AFQT.full }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{Subject, }\SpecialCharTok{{-}}\NormalTok{AFQT, }\SpecialCharTok{{-}}\NormalTok{Gender), }\AttributeTok{scale =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{pc.}\FloatTok{4.}\NormalTok{no.scale  }
\end{Highlighting}
\end{Shaded}

What do you think?

\section{PCA of SVABS}\label{pca-of-svabs}

SVABS contains 10 test scores. How can we use a few summary scores to
capture some main features hidden in the 10 scores? How can we tell who
is good in certain areas? Are there systematic difference between men
and women in the SVABS tests?

We will explore how well PCA can answer all the questions raised.

\subsection{Leading PCs}\label{leading-pcs}

Now bring all the subjects with all 10 test scores in the following
code. We first list PC1 loadings in a decreasing order. Roughly
speaking, the loadings are similar indicating that PC1 captures the
total scores (scaled by the standard deviations for each test.)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pca.all }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(data.full[,  }\FunctionTok{c}\NormalTok{(}\DecValTok{11}\SpecialCharTok{:}\DecValTok{20}\NormalTok{)], }\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{)   }\CommentTok{\# all the tests}
\CommentTok{\# loadings and with test names}
\NormalTok{pca.all.loading }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{tests=}\FunctionTok{row.names}\NormalTok{(pca.all}\SpecialCharTok{$}\NormalTok{rotation), pca.all}\SpecialCharTok{$}\NormalTok{rotation) }
\NormalTok{pca.all.loading }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(tests, PC1, PC2) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{PC1)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             tests   PC1     PC2
## Arith       Arith 0.354  0.0487
## Science   Science 0.352 -0.1419
## Word         Word 0.350  0.0615
## Math         Math 0.336  0.1426
## Parag       Parag 0.326  0.1897
## Elec         Elec 0.324 -0.3353
## Mechanic Mechanic 0.316 -0.3345
## Numer       Numer 0.275  0.4517
## Auto         Auto 0.272 -0.4735
## Coding     Coding 0.234  0.5146
\end{verbatim}

We next look into the PC2 loadings.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# loadings and with test names}
\NormalTok{pca.all.loading }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}\AttributeTok{tests=}\FunctionTok{row.names}\NormalTok{(pca.all}\SpecialCharTok{$}\NormalTok{rotation), pca.all}\SpecialCharTok{$}\NormalTok{rotation) }
\NormalTok{pca.all.loading }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(tests, PC1, PC2, PC3) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{PC2)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##             tests   PC1     PC2     PC3
## Coding     Coding 0.234  0.5146  0.5152
## Numer       Numer 0.275  0.4517  0.3276
## Parag       Parag 0.326  0.1897 -0.3911
## Math         Math 0.336  0.1426 -0.2232
## Word         Word 0.350  0.0615 -0.3429
## Arith       Arith 0.354  0.0487 -0.0919
## Science   Science 0.352 -0.1419 -0.2220
## Mechanic Mechanic 0.316 -0.3345  0.2522
## Elec         Elec 0.324 -0.3353  0.0860
## Auto         Auto 0.272 -0.4735  0.4220
\end{verbatim}

It captures the difference between the total of \texttt{coding},
\texttt{Numer}, \texttt{Parag}, \texttt{Math} and the total of
\texttt{Mechanic}, \texttt{Elec} and \texttt{Auto}.

\textbf{PC1:} Proportional to the total scores. \textbf{PC2:} Difference
between intelligence(such as math/comprehensive understanding?) and
dexterity

\subsection{PVE}\label{pve}

How much variations do leading PCs account for?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(pca.all)}\SpecialCharTok{$}\NormalTok{importance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##                          PC1   PC2    PC3    PC4    PC5    PC6    PC7    PC8
## Standard deviation     2.472 1.193 0.7515 0.7059 0.5578 0.5425 0.4845 0.4758
## Proportion of Variance 0.611 0.142 0.0565 0.0498 0.0311 0.0294 0.0235 0.0226
## Cumulative Proportion  0.611 0.753 0.8097 0.8595 0.8906 0.9201 0.9435 0.9662
##                           PC9   PC10
## Standard deviation     0.4196 0.4027
## Proportion of Variance 0.0176 0.0162
## Cumulative Proportion  0.9838 1.0000
\end{verbatim}

\subsection{Biplot}\label{biplot-1}

To visualize the loadings and the correlations among test scores, here
is the biplot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{biplot}\NormalTok{(pca.all, }\AttributeTok{cex=}\FloatTok{0.5}\NormalTok{, }\AttributeTok{xlim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{08}\NormalTok{, .}\DecValTok{08}\NormalTok{),}
       \AttributeTok{ylim=}\FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{.}\DecValTok{08}\NormalTok{, .}\DecValTok{08}\NormalTok{),}
       \AttributeTok{main=}\StringTok{"PCs for all the 10 tests"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\AttributeTok{h=}\DecValTok{0}\NormalTok{, }\AttributeTok{v=}\DecValTok{0}\NormalTok{, }\AttributeTok{col=}\StringTok{"red"}\NormalTok{, }\AttributeTok{lwd=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-28-1} \end{center}

\subsection{How Gender plays the role
here?}\label{how-gender-plays-the-role-here}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{as.data.frame}\NormalTok{(pca.all}\SpecialCharTok{$}\NormalTok{x) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{gender =}\NormalTok{ data.full}\SpecialCharTok{$}\NormalTok{Gender) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ PC1, }\AttributeTok{y =}\NormalTok{ PC2)) }\SpecialCharTok{+}  \CommentTok{\# pca.all$rotation  try PC3 vs. PC1, no gender effect}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{color =}\NormalTok{ gender)) }\SpecialCharTok{+}
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_hline}\NormalTok{(}\AttributeTok{yintercept =} \DecValTok{0}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"PCs reveal clusters"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_bw}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-29-1} \end{center}

WOW! Males and females are clearly separated by PC2! That implies
females (red circles) are strong in intelligence and males are strong in
dexterity! Does that agree with your intuition?

\texttt{factoextra} package also provides nice PCA biplot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(factoextra)}
\NormalTok{gender }\OtherTok{\textless{}{-}} \FunctionTok{factor}\NormalTok{(data.full}\SpecialCharTok{$}\NormalTok{Gender)}

\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{fviz\_pca\_biplot}\NormalTok{(}
\NormalTok{  pca.all,}
  \AttributeTok{habillage =}\NormalTok{ gender,}
  \AttributeTok{label =} \StringTok{"var"}\NormalTok{,}
  \AttributeTok{repel =} \ConstantTok{TRUE}\NormalTok{,}
  \AttributeTok{pointsize =} \FloatTok{1.3}\NormalTok{,}
  \AttributeTok{alpha.ind =} \FloatTok{0.35}\NormalTok{,}
  \AttributeTok{col.var =} \StringTok{"black"}
\NormalTok{)}

\NormalTok{p }\SpecialCharTok{+} \FunctionTok{theme\_bw}\NormalTok{() }\SpecialCharTok{+} \FunctionTok{ggtitle}\NormalTok{(}\StringTok{"PCA SVABS with Gender"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/pca_biplot_clean-1} \end{center}

\subsection{PVE}\label{pve-1}

How much variability do PC1 and PC2 explain?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pca.all)}\SpecialCharTok{$}\NormalTok{importance)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.2771}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0723}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PC10
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Standard deviation & 2.472 & 1.193 & 0.752 & 0.706 & 0.558 & 0.543 &
0.485 & 0.476 & 0.420 & 0.403 \\
Proportion of Variance & 0.611 & 0.142 & 0.056 & 0.050 & 0.031 & 0.029 &
0.023 & 0.023 & 0.018 & 0.016 \\
Cumulative Proportion & 0.611 & 0.753 & 0.810 & 0.860 & 0.891 & 0.920 &
0.944 & 0.966 & 0.984 & 1.000 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(pca.all)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-31-1} \end{center}

PVE plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pca.all)}\SpecialCharTok{$}\NormalTok{importance[}\DecValTok{2}\NormalTok{, ], }\AttributeTok{pch=}\DecValTok{16}\NormalTok{,}
     \AttributeTok{ylab=}\StringTok{"PVE"}\NormalTok{,}
     \AttributeTok{xlab=}\StringTok{"Number of PCs"}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"PVE scree plot of PCA with all 10 scores "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-32-1} \end{center}

We see that PC1 accounts for 61\% of the total variation in the 10
scores following by PC2 with 14\%. With only two leading PCs we capture
about 75\% of the variance.

The scree plot of CPVE

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(}\FunctionTok{summary}\NormalTok{(pca.all)}\SpecialCharTok{$}\NormalTok{importance[}\DecValTok{3}\NormalTok{, ], }\AttributeTok{pch=}\DecValTok{16}\NormalTok{,}
     \AttributeTok{ylab=}\StringTok{"Cumulative PVE"}\NormalTok{,}
     \AttributeTok{xlab=}\StringTok{"Number of PCs"}\NormalTok{,}
     \AttributeTok{main=}\StringTok{"Scree plot of Cumulative PVE "}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-33-1} \end{center}

75\% of the total variability are explained or captured by the two
leading PCs.

To capture the main features of all the tests we could use two summary
scores

\begin{itemize}
\tightlist
\item
  PC1: Total scores (weighted)
\item
  PC2: Difference between intelligence(such as math/words?) and
  dexterity
\end{itemize}

\section{Recap}\label{recap}

Principal Component Analysis finds linear combinations of the variables
that capture the most information contained in the full data. We may
even find some striking relationships among variables through a few PCs.
It is often useful to reveal group information or to identify clusters.
All PCs are orthogonal which can be an advantageous property when we use
them as predictors. The drawback is that we may lose the interpretation
based on the original variables.

\section{Appendix 1: PC definitions via maximizing the variance of
linear
combinations.}\label{appendix-1-pc-definitions-via-maximizing-the-variance-of-linear-combinations.}

In this section we write formal definition of PCs with four
\texttt{AFQT} tests.

\subsection{First Principal Component}\label{first-principal-component}

We are looking for a linear transformation \(Z_1\) of
\(X_1=\texttt{Word}\), \(X_2=\texttt{Parag}\), \(X_3=\texttt{Math}\),
and \(X_4=\texttt{Arith}\) to have the max variance.

\[Z_1=\phi_{11}X_1 + \phi_{21}X_2 + \phi_{31}X_3 + \phi_{41}X_4\] such
that
\[\max_{\phi_{11}^2+ \phi_{21}^2 +\phi_{31}^2+ \phi_{41}^2=1}\mathrm{Var}(Z_1)\]

\subsection{Second Principal
Component}\label{second-principal-component}

Similarly, we are looking for another linear transformation \(Z_2\) of
\(X_1=\texttt{Word}\), \(X_2=\texttt{Parag}\), \(X_3=\texttt{Math}\),
and \(X_4=\texttt{Arith}\) to have the max variance and \(Z_1\) is
orthogonal to \(Z_2\).

\[Z_2 =\phi_{12}X_1 + \phi_{22}X_2 + \phi_{32}X_3 + \phi_{42}X_4\] such
that
\[\max_{\phi_{12}^2+ \phi_{22}^2 +\phi_{32}^2+ \phi_{42}^2=1}\mathrm{Var}(Z_2)\]

By definitions we know two sets of loadings are perpendicular: or
\((\phi_{11}, \phi_{21}, \phi_{31}, \phi_{41})\) and
\((\phi_{12}, \phi_{22}, \phi_{32}, \phi_{42})\) are orthogonal.

\subsection{More PC components}\label{more-pc-components}

We keep going to obtain \(Z_3\), and \(Z_4\).

\section{Appendix 2: PCA and Eigen decomposition Correlation
Matrix}\label{appendix-2-pca-and-eigen-decomposition-correlation-matrix}

How to get all the loadings, PCs? There are elegant, simple mathematics
behind it.

To find the PC loadings we want to maximize the variance of the linear
combination of the variables. Let \(X=(X_1, X_2, \ldots, X_p)\) be the
design matrix. We simply list all values of first variable \(X_1\) for
all subjects, and with similar ways to list \(X_2\) and so on. Notice
that the design matrix is an \(n \times p\) matrix.

It is easy to show that the PC loadings are nothing but
eigenvectors/values of \(corr(X_1,X_2,\ldots, X_p)=X^\top X/(n-1)\) (if
centered and scaled) or \(cov(X_1,X_2)\) (unscaled).

Let us use \texttt{data.AFQT} which has 50 people and 4 variables
\texttt{Word}, \texttt{Parag}, \texttt{Math} and \texttt{Arith}.

Eigenvectors of \texttt{cor(data.AFQT)} give us the loadings with
ordered PC1, PC2 and so on. Eigenvalues are the variances of PC1, PC2
and so on.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{PC.eig }\OtherTok{\textless{}{-}} \FunctionTok{eigen}\NormalTok{(}\FunctionTok{cor}\NormalTok{(data.AFQT))}
\NormalTok{PC.eig}\SpecialCharTok{$}\NormalTok{vectors   }\CommentTok{\# Loadings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]   [,2]   [,3]   [,4]
## [1,] 0.509  0.442  0.365  0.642
## [2,] 0.500  0.546 -0.308 -0.598
## [3,] 0.496 -0.483 -0.652  0.310
## [4,] 0.494 -0.523  0.589 -0.368
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{EigenVectors }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(PC.eig}\SpecialCharTok{$}\NormalTok{vectors)}
\FunctionTok{names}\NormalTok{(EigenVectors) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Eigvec.1"}\NormalTok{, }\StringTok{"Eigvec.2"}\NormalTok{, }\StringTok{"Eigvec.3"}\NormalTok{, }\StringTok{"Eigvec.4"}\NormalTok{)}
\CommentTok{\# create eigen vectors }
\NormalTok{PC.eig}\SpecialCharTok{$}\NormalTok{values    }\CommentTok{\# Variances of each PCs }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.061 0.488 0.271 0.180
\end{verbatim}

Let us check against \texttt{prcomp()} output:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We use prcomp() here}
\NormalTok{PC }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(data.AFQT, }\AttributeTok{scale=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{PC  }\CommentTok{\# should be exactly same as PCs from eigen decomposition (up to the sign)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Standard deviations (1, .., p=4):
## [1] 1.750 0.699 0.521 0.424
## 
## Rotation (n x k) = (4 x 4):
##         PC1    PC2    PC3    PC4
## Word  0.509  0.442 -0.365 -0.642
## Parag 0.500  0.546  0.308  0.598
## Math  0.496 -0.483  0.652 -0.310
## Arith 0.494 -0.523 -0.589  0.368
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{phi }\OtherTok{\textless{}{-}}\NormalTok{ PC}\SpecialCharTok{$}\NormalTok{rotation}
\NormalTok{phi}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         PC1    PC2    PC3    PC4
## Word  0.509  0.442 -0.365 -0.642
## Parag 0.500  0.546  0.308  0.598
## Math  0.496 -0.483  0.652 -0.310
## Arith 0.494 -0.523 -0.589  0.368
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cbind}\NormalTok{(EigenVectors, PC}\SpecialCharTok{$}\NormalTok{rotation) }\CommentTok{\# Putting the first PC together}
\CommentTok{\# one from eigen{-}values???the other from prcomp(). }
\CommentTok{\# They should be exactly the same (to the sign) and they are the same.}
\end{Highlighting}
\end{Shaded}

Eigenvectors and PC rotations are the same up to a sign difference. Are
you convinced that PC loadings are the same as eigenvectors of
correlation matrix of variables?

\section{Appendix 3: PCA and SVD}\label{appendix-3-pca-and-svd}

A matrix \(X\) can be decomposed by Singular Value Decomposition (SVD).
PCs can be obtained through SVD. SVD is very useful in applications,
e.g., matrix completion, recommendation systems. Assume that \(X\) is
centered and scaled.

Fact: any matrix X can be decomposed as follows:
\[X_{n \times  p}=U_{n\times p} D_{p \times p} V^\top_{n \times p}\]
Here \(U\) is column orthonormal and it is call left singular vector for
each column. \(V\) is right singular vector of orthonormal matrix. \(D\)
is a diagonal matrix with decreasing values \(d_1 > d_2,...>d_p\) and it
is call singular values accordingly.

\subsection{Properties of SVD}\label{properties-of-svd}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Matrix of rank 1 representation
\end{enumerate}

Rewrite the matrix SVD to a sum of rank 1 matrices

\[X_{n \times  p} = d_1 u_1 v_1^\top + d_2 u_2 v_2^\top + \ldots d_p u_p v_p^\top\]
Here \(u_1, \ldots, u_p\) are columns of \(U\) and \(v_1, \ldots, v_p\)
are columns of \(V\) with norm 1, i.e.~\(\|u_j\|_2 = 1\) for
\(j = 1, \ldots, p\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Since \(d_1, \ldots, d_p\) are decreasing, we may take top singular
  vectors to approximate the matrix \(X\).
\item
  It is easy to see \(v_1, \ldots, v_p\) is the eigenvectors of
  \(X^\top X\) and \(d_1^2, \ldots, d_p^2\) are corresponding
  eigenvalues. So the right singular vectors \(v_1, \ldots, v_p\) give
  us the loadings for PCs.
\end{enumerate}

It is easy to prove by plugging in \(X = U D V^\top\) and notice that
\(U\) and \(V\) are orthonormal, i.e., \(U^\top U = I\) and
\(V^\top V = I\). Immediately we get
\[\frac{X^\top X}{n-1} V = \frac{1}{n-1} V D U^\top U D V^\top V = V \frac{D^2}{n-1}\]

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \(X V = U D\) that means \(\text{PC scores} = U D\).
\end{enumerate}

How beautiful!

\subsection{Compare PCA and SVD}\label{compare-pca-and-svd}

Let us verify this using function \texttt{svd()}. We use
\texttt{data.AFQT} again.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data.AFQT }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.sub }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(Word, Parag, Math, Arith)}
\NormalTok{data.AFQT.center.scale }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(data.AFQT, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{pc}\FloatTok{.4} \OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(data.AFQT.center.scale, }\AttributeTok{scale =} \ConstantTok{TRUE}\NormalTok{)}
\NormalTok{AFQT.svd }\OtherTok{\textless{}{-}} \FunctionTok{svd}\NormalTok{(data.AFQT.center.scale)}
\FunctionTok{names}\NormalTok{(AFQT.svd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "d" "u" "v"
\end{verbatim}

\subsubsection{PC loadings}\label{pc-loadings}

Right singular vectors \(v_1, \ldots, v_p\) are PC loadings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{AFQT.svd}\SpecialCharTok{$}\NormalTok{v}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##       [,1]   [,2]   [,3]   [,4]
## [1,] 0.509  0.442 -0.365 -0.642
## [2,] 0.500  0.546  0.308  0.598
## [3,] 0.496 -0.483  0.652 -0.310
## [4,] 0.494 -0.523 -0.589  0.368
\end{verbatim}

Compare with PC loadings

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{rotation}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##         PC1    PC2    PC3    PC4
## Word  0.509  0.442 -0.365 -0.642
## Parag 0.500  0.546  0.308  0.598
## Math  0.496 -0.483  0.652 -0.310
## Arith 0.494 -0.523 -0.589  0.368
\end{verbatim}

\subsubsection{PC scores}\label{pc-scores}

Let's take a look at PC1 first. \(PC1 = d_1 u_1\)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{d1.u1 }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.svd}\SpecialCharTok{$}\NormalTok{d[}\DecValTok{1}\NormalTok{] }\SpecialCharTok{*}\NormalTok{ AFQT.svd}\SpecialCharTok{$}\NormalTok{u[, }\DecValTok{1}\NormalTok{]}
\NormalTok{pc1 }\OtherTok{\textless{}{-}} \FunctionTok{prcomp}\NormalTok{(data.AFQT.center.scale)}\SpecialCharTok{$}\NormalTok{x[, }\DecValTok{1}\NormalTok{]}
\FunctionTok{cbind}\NormalTok{(d1.u1, pc1)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     d1.u1    pc1
## p1 -0.920 -0.920
## p2  0.611  0.611
## p3 -0.312 -0.312
## p4  3.069  3.069
## p5  2.818  2.818
\end{verbatim}

We compare PC scores computed by \(UD\) and by \texttt{prcomp()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(AFQT.svd}\SpecialCharTok{$}\NormalTok{u }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{(AFQT.svd}\SpecialCharTok{$}\NormalTok{d) }\SpecialCharTok{{-}}\NormalTok{ pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{x)[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{,]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          PC1       PC2       PC3      PC4
## p1 -2.33e-15 -6.25e-16 -6.66e-16 2.22e-16
## p2 -3.33e-16 -6.66e-16  0.00e+00 8.85e-17
## p3  3.89e-16  4.44e-16  1.39e-16 2.22e-16
## p4  0.00e+00  2.84e-16  3.89e-16 1.28e-15
## p5  0.00e+00  0.00e+00  4.72e-16 1.14e-15
\end{verbatim}

\subsubsection{Variance of PC scores}\label{variance-of-pc-scores}

Variance of PC scores are \(d_i^2/(n-1)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var.pc }\OtherTok{\textless{}{-}}\NormalTok{ (pc}\FloatTok{.4}\SpecialCharTok{$}\NormalTok{sdev)}\SpecialCharTok{\^{}}\DecValTok{2}
\NormalTok{var.pc.svd }\OtherTok{\textless{}{-}}\NormalTok{ AFQT.svd}\SpecialCharTok{$}\NormalTok{d}\SpecialCharTok{\^{}}\DecValTok{2}\SpecialCharTok{/}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(data.AFQT)}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}
\FunctionTok{cbind}\NormalTok{(var.pc, var.pc.svd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      var.pc var.pc.svd
## [1,]  3.061      3.061
## [2,]  0.488      0.488
## [3,]  0.271      0.271
## [4,]  0.180      0.180
\end{verbatim}

\section{Appendix 4: Missing values and recommender
system}\label{appendix-4-missing-values-and-recommender-system}

Often datasets have missing values, which can be a nuisance. Many data
analysis functions will simply delete the rows with missing values.
Other examples such as recommender systems where based on what are known
we may come up with informative recommendations for the missing cells.
For instance, we may form a matrix \(X\) of the movie ratings that \(n\)
customers have given to the entire catalog of \(p\) movies. Most of the
matrix will be missing, since no customer will have seen and rated more
than a tiny fraction of the catalog.

\subsection{Case study: recommender system to provide favorite
movies}\label{case-study-recommender-system-to-provide-favorite-movies}

Let us look at the real dataset collected from
\href{https://grouplens.org/datasets/movielens}{MovieLens} website. We
use the small one which contains \(100,836\) ratings to \(p=9,742\)
movies by \(n=610\) users. The data contains variable \texttt{userId},
\texttt{movieId}, \texttt{rating} and \texttt{timestamp}. Each row is
one rating, which is what we have termed a long form. \textbf{We hope to
fill in all the missing cells so that we can recommend a user movies
that he/she is very likely enjoy watching.}

\subsubsection{A quick EDA}\label{a-quick-eda}

Read the data and examine the format of the variables. We format the
time first into readable time scale.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movieLens\_raw }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{\textquotesingle{}MovieLens.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 100836 Columns: 4
## -- Column specification --------------------------------------------------------
## Delimiter: ","
## dbl (4): userId, movieId, rating, timestamp
## 
## i Use `spec()` to retrieve the full column specification for this data.
## i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(movieLens\_raw)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "userId"    "movieId"   "rating"    "timestamp"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dim}\NormalTok{(movieLens\_raw)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 100836      4
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(movieLens\_raw, }\DecValTok{2}\NormalTok{)  }\CommentTok{\# notice timestmp is a real number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 4
##   userId movieId rating timestamp
##    <dbl>   <dbl>  <dbl>     <dbl>
## 1      1       1      4 964982703
## 2      1       3      4 964981247
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# convert timestamp into readable time using \textasciigrave{}lubridate::as\_datetime()\textasciigrave{}}
\NormalTok{movieLens\_raw }\OtherTok{\textless{}{-}}\NormalTok{ movieLens\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{time =} \FunctionTok{as\_datetime}\NormalTok{(timestamp))}
\FunctionTok{head}\NormalTok{(movieLens\_raw, }\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 5
##    userId movieId rating timestamp time               
##     <dbl>   <dbl>  <dbl>     <dbl> <dttm>             
##  1      1       1      4 964982703 2000-07-30 18:45:03
##  2      1       3      4 964981247 2000-07-30 18:20:47
##  3      1       6      4 964982224 2000-07-30 18:37:04
##  4      1      47      5 964983815 2000-07-30 19:03:35
##  5      1      50      5 964982931 2000-07-30 18:48:51
##  6      1      70      3 964982400 2000-07-30 18:40:00
##  7      1     101      5 964980868 2000-07-30 18:14:28
##  8      1     110      4 964982176 2000-07-30 18:36:16
##  9      1     151      5 964984041 2000-07-30 19:07:21
## 10      1     157      5 964984100 2000-07-30 19:08:20
\end{verbatim}

How many unique users and movies?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of unique movies}
\FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(movieLens\_raw}\SpecialCharTok{$}\NormalTok{movieId))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 9724
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# number of unique users}
\FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(movieLens\_raw}\SpecialCharTok{$}\NormalTok{userId))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 610
\end{verbatim}

How many movies each user rated? How many movies each user rated on
average?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{num\_by\_user }\OtherTok{\textless{}{-}}\NormalTok{ movieLens\_raw }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(userId) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{()) }

\FunctionTok{hist}\NormalTok{(num\_by\_user}\SpecialCharTok{$}\NormalTok{n, }
     \AttributeTok{main =} \StringTok{"Histogram of number of movies each user rated"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-43-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# the user who rates the most movies}
\FunctionTok{max}\NormalTok{(num\_by\_user}\SpecialCharTok{$}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2698
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# average number of rated movies by user}
\FunctionTok{mean}\NormalTok{(num\_by\_user}\SpecialCharTok{$}\NormalTok{n)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 165
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# average proportion of rated movies by user }
\FunctionTok{mean}\NormalTok{(num\_by\_user}\SpecialCharTok{$}\NormalTok{n)}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(}\FunctionTok{unique}\NormalTok{(movieLens\_raw}\SpecialCharTok{$}\NormalTok{movieId))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.017
\end{verbatim}

Let's first look at the histogram of ratings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movieLens\_raw }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(rating) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{n =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 10 x 2
##    rating     n
##     <dbl> <int>
##  1    0.5  1370
##  2    1    2811
##  3    1.5  1791
##  4    2    7551
##  5    2.5  5550
##  6    3   20047
##  7    3.5 13136
##  8    4   26818
##  9    4.5  8551
## 10    5   13211
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{hist}\NormalTok{(movieLens\_raw}\SpecialCharTok{$}\NormalTok{rating)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{lecture_PCA_files/figure-latex/unnamed-chunk-44-1} \end{center}

\subsubsection{Data preparation}\label{data-preparation}

We need to first convert from long to wide format using
\texttt{pivor\_wider()} so that

\begin{itemize}
\tightlist
\item
  each row is the ratings of movies from one user
\item
  each column is the ratings of one movie from users
\item
  we should expect many entries are \texttt{NA}
\end{itemize}

We convert the data from long to wide so that it becomes a matrix (with
\texttt{NA}s) to apply matrix completion algorithms (such as
\texttt{softImpute()} we will use later).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movieLens }\OtherTok{\textless{}{-}}\NormalTok{ movieLens\_raw }\SpecialCharTok{\%\textgreater{}\%} 
  \CommentTok{\# select({-}timestamp, {-}time) \%\textgreater{}\% }
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{userId =} \FunctionTok{paste0}\NormalTok{(}\StringTok{\textquotesingle{}user\textquotesingle{}}\NormalTok{, userId)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{pivot\_wider}\NormalTok{(}\AttributeTok{id\_cols =}\NormalTok{ userId,  }\CommentTok{\# each row}
              \AttributeTok{names\_from =}\NormalTok{ movieId, }\AttributeTok{names\_prefix =} \StringTok{\textquotesingle{}movie\textquotesingle{}}\NormalTok{, }
              \AttributeTok{values\_from =}\NormalTok{ rating)}

\DocumentationTok{\#\# pivot back to longer format}
\CommentTok{\# movieLens \%\textgreater{}\%}
\CommentTok{\#   pivot\_longer(cols = starts\_with("movie"),}
\CommentTok{\#                names\_to = "movieId",}
\CommentTok{\#                values\_to = "rating")}

\NormalTok{movieLens[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{5}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 5
##   userId movie1 movie3 movie6 movie47
##   <chr>   <dbl>  <dbl>  <dbl>   <dbl>
## 1 user1       4      4      4       5
## 2 user2      NA     NA     NA      NA
## 3 user3      NA     NA     NA      NA
## 4 user4      NA     NA     NA       2
## 5 user5       4     NA     NA      NA
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sum}\NormalTok{(}\FunctionTok{is.na}\NormalTok{(movieLens))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5830804
\end{verbatim}

Digital streaming services like Netflix and Amazon use data about the
content that a customer has viewed in the past, as well as data from
other customers, to suggest other content for the customer. If we can
impute the missing values well, we will have an idea of what each
customer will think of movies they have not yet seen and be able to
suggest a movie that a particular customer might like. Principal
components analysis would be an useful tool to impute the missing
values.

\subsection{Objective function}\label{objective-function}

Given the data matrix \(X \in \mathbb R^{n\times p}\), some of the
observations \(x_{ij}\) are missing and \(\mathcal O\) denotes the set
of all observed pairs of indices \((i, j)\), a subset of the possible
\(n \times p\) pairs. Given a pre-determined number \(M\) of components,
our goal is to find low rank approximations
\(U \in \mathbb R^{n\times M}\) and \(V \in \mathbb R^{M\times p}\)
which minimize
\[ \sum_{(i,j)\in\mathcal O} \left( x_{ij} - \sum_{m=1}^M d_m u_{im} v_{jm} \right)^2. \]

In other words, we are trying to find the best approximation
\(\widehat U \in \mathbb R^{n\times M}\) and
\(\widehat V \in \mathbb R^{M\times p}\) based on observed entries. Once
we solve this problem, we can estimate a missing observation \(x_{ij}\)
using
\[ \widehat x_{ij} = \sum_{m=1}^M d_m \widehat u_{im} \widehat v_{jm} \]

where \(\widehat u_{im}\) and \(\widehat v_{jm}\) are the \((i, m)\) and
\((j, m)\) elements, respectively, of the best approximation
\(\widehat U \in \mathbb R^{n\times M}\) and
\(\widehat V \in \mathbb R^{M\times p}\).

\subsection{Algorithm}\label{algorithm}

It turns out that solving this problem exactly is difficult, unlike in
the case of complete data: the vanilla PCA no longer applies. However,
many researchers found that iteratively applying the vanilla PCA (or
SVD) provides a good solution. The R package named \texttt{softImpute}
yields a nice approximation, based on this simple idea.

\subsection{Recommender system}\label{recommender-system}

As we have seen before, the recommender system aims to impute the
missing values of rating. The key idea is that the set of movies which
the \(i\)th customer has seen will overlap with those which other
customers have seen. Furthermore, some of those other customers will
have similar movie preferences to the \(i\)th customer. Thus, we may use
similar customers' movies ratings that the \(i\)th customer has not seen
to predict whether the \(i\)th customer will like those movies.

We can use the same imputing algorithm to predict the \(i\)th customer's
rating. \texttt{softImpute}. More concretely, the \(i\)th customer's
rating would be
\[ \widehat x_{ij} = \sum_{m=1}^M \widehat u_{im} \widehat v_{jm} \]

Here, \(\widehat u_{im}\) represents the strength with which the \(i\)th
user belongs to the \(m\)th clique, where a clique is a group of
customers that enjoys movies of the \(m\)th genre. Not only that,
\(\widehat v_{jm}\) represents the strength with which the \(j\)th movie
belongs to the \(m\)th genre.

We implement recommender system through \texttt{softImpute} package with
the choice \(M=5\), the number of hidden components. The result is
stored as an \texttt{svd} object named \texttt{fit} with components
\texttt{u}, \texttt{d}, and \texttt{v}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit }\OtherTok{\textless{}{-}}\NormalTok{ movieLens }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{userId) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{as.matrix.data.frame}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{softImpute}\NormalTok{(}\AttributeTok{type=}\StringTok{\textquotesingle{}als\textquotesingle{}}\NormalTok{, }\AttributeTok{rank.max=}\DecValTok{5}\NormalTok{)}

\FunctionTok{str}\NormalTok{(fit) }\CommentTok{\#hist(fit$d)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## List of 3
##  $ u: num [1:610, 1:5] -0.0654 -0.0264 -0.0177 -0.0495 -0.0229 ...
##  $ d: num [1:5] 3288 1362 1180 990 955
##  $ v: num [1:9724, 1:5] -0.027 -0.017 -0.027 -0.027 -0.0293 ...
##  - attr(*, "lambda")= num 0
##  - attr(*, "call")= language softImpute(x = ., rank.max = 5, type = "als")
\end{verbatim}

The following code yields the predicted rating matrix.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{movieLens\_pred }\OtherTok{\textless{}{-}}\NormalTok{ fit}\SpecialCharTok{$}\NormalTok{u }\SpecialCharTok{\%*\%} \FunctionTok{diag}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{d) }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(fit}\SpecialCharTok{$}\NormalTok{v)  }\CommentTok{\# may try complete() }
\FunctionTok{colnames}\NormalTok{(movieLens\_pred) }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(movieLens)[}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}

\NormalTok{movieLens\_pred }\OtherTok{\textless{}{-}} \FunctionTok{as\_tibble}\NormalTok{(movieLens\_pred) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_column}\NormalTok{(}\AttributeTok{userId =} \FunctionTok{pull}\NormalTok{(movieLens, userId), }\AttributeTok{.before =} \StringTok{\textquotesingle{}movie1\textquotesingle{}}\NormalTok{)}
\NormalTok{movieLens\_pred[}\DecValTok{1}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{] }\CommentTok{\# movieLens[1, 1:20]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 20
##   userId movie1 movie3 movie6 movie47 movie50 movie70 movie101 movie110 movie151
##   <chr>   <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>    <dbl>    <dbl>    <dbl>
## 1 user1    4.49   3.93   4.45    4.30    5.26    3.04     4.84     4.49     4.58
## # i 10 more variables: movie157 <dbl>, movie163 <dbl>, movie216 <dbl>,
## #   movie223 <dbl>, movie231 <dbl>, movie235 <dbl>, movie260 <dbl>,
## #   movie296 <dbl>, movie316 <dbl>, movie333 <dbl>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#sum(is.na(movieLens\_pred))   \#sum(is.na(movieLens))}
\end{Highlighting}
\end{Shaded}


\end{document}
